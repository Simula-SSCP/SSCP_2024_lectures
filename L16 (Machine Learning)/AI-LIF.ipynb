{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7de3ab1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb AI_LIF.do.txt  -->\n",
    "\n",
    "## From Spiking Neurons to Deep Neural Networks\n",
    "<!-- FIGURE:[figures/header-image.png, height=200, alt=\"Header Image\", frac=0.8] An Artificial Intelligent's impression of what spiking neurons that start to form a complex network look like. <div id=\"fig:HeaderImg\"></div> -->\n",
    "\n",
    "We think of neurons as the basic information processing units in our brain.\n",
    "Similarly, artificial neurons, that we got to know in the previous lectures, constitute a fundamental building block of artificial neural networks.\n",
    "Albeit having some things in common, artifical and biological neurons operate fundamentally differently.\n",
    "If anything, artifical neurons can be thought of as an oversimplification of the basic integrating characteristic of their biological counterparts.\n",
    "\n",
    "On the other hand, this oversimplication also comes with significant benefits such as scalability.\n",
    "For a neural network model to learn cognitive functionality, upscaling appears to make a larger difference compared with physiological plausibility.\n",
    "Additionally, we know that Multi Layer Perceptrons (MLPs) can approximate arbitrary functions.\n",
    "In turn, this means MLPs can model the more sophisticated processing happening in biological neurons, it just might take more than only one artificial neuron. \n",
    "However, this way we can keep computationally beneficial properties, associated with MLPs, that are not present in other modeling approaches.\n",
    "\n",
    "In this lecture, we want to use deep artificial neural networks to model complex dynamical systems, such as biological neurons.\n",
    "This in mind, we will learn more about one particular physiological model for information processing in biological neurons: the **Integrate and Fire Neuron**, and how we can approximate them using deep artificial neural networks.\n",
    "We will learn that biological neurons feature much richer processing capabilities than their artificial counterparts, but that deep neural networks are able to approximate them very accurately. \n",
    "This result suggests that deep neural networks not only constitute a valid model for biological neurons, but using them also enables the field of computational neuroscience to leverage the enormous toolset and methodology that is available in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34738531",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The Integrate and Fire Neuron\n",
    "\n",
    "In principle, neurons receive information through connections to other neurons (synapses) that are located at the end of their dendrites.\n",
    "In fact, neurons receive information from a bunch of neighbours, which is why we call it the dendritic tree.\n",
    "Neurons accumulate all the dendritic inputs, process them and generate a signal that is output through synapses at the end of the axon.\n",
    "[Figure 1](#fig:abstract_neuron_cartoon) shows a schematic of that abstract view on a neuron.\n",
    "In summary, that is the simple input-processing-output principle neural processing units operate on.\n",
    "Actually, the neural dynamic behaviour is way more sophisticated, but for this lecture, we will continue with that simplified perspective.\n",
    "\n",
    "<!-- dom:FIGURE:[figures/abstract_neuron_cartoon.png, height=200, alt=\"Cartoon of an abstract neuron\", frac=0.8] Schematic of an abstracted neuron. <div id=\"fig:abstract_neuron_cartoon\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:abstract_neuron_cartoon\"></div>\n",
    "\n",
    "<img src=\"figures/abstract_neuron_cartoon.png\" height=\"200\" alt=\"Cartoon of an abstract neuron\"><p style=\"font-size: 0.9em\"><i>Figure 1: Schematic of an abstracted neuron.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "**Action potentials become spike trains.** So far, we have studied physiological models that explain how biophysically the action potential is mechanistically created in the axon.\n",
    "Examples of such models are the __Hodgkin-Huxley__ or the __Fitz Hugh-Nagamo__ model.\n",
    "The output signal of a neuron comprises a series of characteristic action potential profiles in time.\n",
    "However, the shape of the profile itself does not change much.\n",
    "Therefore, it can not be the action potential profile that encodes information, but the relative timing between action potentials.\n",
    "The profile itself is only a result of how information transmission is mechanistically realised in that system.\n",
    "As a consequence, it only contains information about the underlying biophysical processes involved.\n",
    "\n",
    "Because the form of the action potential is irrelevant when we are interested in an information based view, we reduce it to the event of onset and call it a spike.\n",
    "In this model, the neural output signal comprises a time series of such spikes: a spike train. \n",
    "Mathematically, we model a spike train by a superposition of time translated delta functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45129d56",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    s(t) = \\sum_{i} \\delta(t - t_{i}) \\quad \\textrm{.}\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198bef",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $t_{i}$ denote the absolute time points at which an action potential is triggered.\n",
    "\n",
    "<!-- dom:FIGURE:[figures/spike_train_abstraction.png, height=200, alt=\"Visualisation of spike train abstraction.\", frac=0.8] The potential profile of the action potential does not carry any information, only the initiation times do. Therefore, we can abstract the complex shape of the axon's potential time series by a series of events: the spike train. <div id=\"fig:spike_train_abstraction\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:spike_train_abstraction\"></div>\n",
    "\n",
    "<img src=\"figures/spike_train_abstraction.png\" height=\"200\" alt=\"Visualisation of spike train abstraction.\"><p style=\"font-size: 0.9em\"><i>Figure 2: The potential profile of the action potential does not carry any information, only the initiation times do. Therefore, we can abstract the complex shape of the axon's potential time series by a series of events: the spike train.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "Note that the spike train model exhibits several mathematical pathologies, such as divergent derivatives. \n",
    "Nevertheless, the spike train model turns out to be a useful simplification when handled with care.\n",
    "But how are the spike trains generated from the signals?\n",
    "\n",
    "**Generative Models (of spike trains).** Based on our current perspective, the neuron's main function lies in accumulating and processing signals.\n",
    "In this section, we will discuss how processing can be realised mechanistically.\n",
    "A common class of approximate neuron models, useful for modeling spike train signals, are so called **Integrate and Fire (IF)** models. \n",
    "\n",
    "Because the shape of the action potential is only weakly affected by signal and noise, it is more important for us to know when the action potential is initiated. \n",
    "The action potential gets inevitably initiated once stimuli and fluctutations have driven the membrane potential $u(t)$ beyond a critical value $u_{\\textrm{thresh}}$ and the neuron fires a spike.\n",
    "This defines a fire criterion, or rule.\n",
    "After spiking, the membrane potential is brought back to hyperpolarised subthreshold values, the reset potential $u_{\\textrm{r}} < u_{\\textrm{thresh}}$.\n",
    "In summary, this defines a **Fire-and-Reset rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b2158",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        u(t) &>& u_{\\textrm{thresh}} \\quad \\Rightarrow \\quad \\textrm{spike at time} \\: t \\\\\n",
    "        u(t + \\tau_{\\textrm{ref}}) &:=& u_{\\textrm{r}} \\quad \\textrm{.}\n",
    "    \\end{split}\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523490a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\tau_{\\textrm{ref}}$ denotes the absolute refractory period which takes into account that the neuron is not excitable for a certain amount of time after undergoing hyperpolarisation.\n",
    "\n",
    "<!-- dom:FIGURE:[figures/integrate_and_fire.png, height=200, alt=\"Typical time series of an IF model.\", frac=0.8] Typical membran potential time series of an IF model. <div id=\"fig:integrate_and_fire\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:integrate_and_fire\"></div>\n",
    "\n",
    "<img src=\"figures/integrate_and_fire.png\" height=\"200\" alt=\"Typical time series of an IF model.\"><p style=\"font-size: 0.9em\"><i>Figure 3: Typical membran potential time series of an IF model.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "Note that, in general, the characteristic threshold and reset potentials do not need to be restricted by single constant values, but are also functions themselves.\n",
    "In addition to a **Fire-and-Reset rule**, an **Integrate and Fire** model is defined its characteristic sub-threshold dynamic.\n",
    "This dynamic is generally non-linear and is driven by the external stimulus current $I(t)$ that respects signal and noise sources.\n",
    "In simple models, the sub-threshold dynamics are often described by a single **nonlinear Ordinary Differential Equation (ODE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451f9f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tau_{m} \\frac{d}{dt} u(t) = f(u) + I(t) \\quad \\textrm{.}\n",
    "\\label{_auto3} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ec3e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can choose different kinds of models for our driving source $I(t)$.\n",
    "To model spontaneous activity, for example, we usually model the driving current by a constant mean current $\\mu$ and additive random fluctuations $\\xi(t)$ with zero mean, $\\delta$ correlation and variance $2\\, D$ (white Gaussian noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c8b7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    I(t) = \\mu + \\sqrt{2\\, D} \\, \\xi(t) \\quad \\textrm{.}\n",
    "\\label{_auto4} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5159e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $D$ controlles the noise intensity.\n",
    "This particular model represents common experimental setups quite well.\n",
    "On the other hand, we might be interested in what output a neuron produces when it receives a certain input.\n",
    "For this, let us assume the neuron receives $N_{\\textrm{syn}}$ input signals $i_{s}(t)$ through its dendritic synapses.\n",
    "Through learning, individual synapses become differently pronounced by neuro-plastistic processes. \n",
    "As a result, we can think of different synapses having different coupling strenghts $w_{s}$.\n",
    "We, therefore, model the total incoming signal that the neuron \"sees\" as the weighted sum of all inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cf783",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    I(t) = \\sum_{s = 1}^{N_{\\textrm{syn}}} w_{s}\\, i_{s}(t) \\quad \\textrm{.}\n",
    "\\label{_auto5} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b2f93",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, we model all signals as spike trains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7624ca3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    i_{s}(t) = \\sum_{f} \\delta(t - t^{f}_{s}) \\quad \\textrm{.}\n",
    "\\label{_auto6} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f9df1",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this setup, we can not only control the parameters of our **Integrate and Fire** model, but also its input by choosing the spike times $t^{f}_{s}$.\n",
    "In the following setup, we consider a Poisson point process of constant firing rate $f_{i}$ to draw the spike times for each source spike train.\n",
    "\n",
    "**The Leaky Integrate and Fire Neuron (LIF).** A simple, linear integrate and fire model features a constant leak current. \n",
    "It is thus called the leaky integrate and fire neuron model.\n",
    "The corresponding differential equation of the membrane potential reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c132059",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:LIF\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tau \\frac{d}{dt} u(t) = - (u(t) - u_{\\textrm{reset}}) + R \\, I(t)\n",
    "    \\label{eq:LIF} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a02d71",
   "metadata": {
    "editable": true
   },
   "source": [
    "with a Fire-and-Reset Rule: if $u(t) > u_{\\textrm{thresh}}$, $u(t) := u_{\\textrm{reset}}$.\n",
    "Formally, we can interpret the sub-threshold dynamic equation above as a Langevin equation of an overdamped particle in an external potential $U(u)$ \n",
    "that is subject to a random force $F_{\\text{R}}(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f96bfe",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tau \\frac{d}{dt} u(t) = - U^{\\prime}(u) + F_{\\text{R}}(t).\n",
    "\\label{_auto7} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37447f01",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since one identifies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eccbf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    - U^{\\prime}(u) = - (u - u_{\\textrm{reset}}),\n",
    "\\label{_auto8} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa2782",
   "metadata": {
    "editable": true
   },
   "source": [
    "$U(u)$ is a parabola around $u_{\\textrm{rest}}$.\n",
    "By this analogy, we can actually qualitatively solve the equation by using out intuition, without actually solving it.\n",
    "Take a look at the Figure below and imagine a ball that rolls in a parabolic potential landscape.\n",
    "The Langevin equation as it is stated above is only valid in the overdamped regime, where the ball experiences a friction forge due to a very viscous fluid.\n",
    "For example, imagine the ball submerged in honey.\n",
    "Since the honey damps the ball, it can not build up momentum and the intertial term vanishes.\n",
    "No matter where the ball starts, the external potential directs it back to the minimum $u_{\\text{rest}}$.\n",
    "In addition, it experiences undirected kicks as a result of the random force $F_{\\text{R}}(t)$ that deviate the back away from $u_{rest}$.\n",
    "As soon as $F_{\\text{R}}(t)$ drove the ball beyond the threshold voltage $u_{\\text{thresh}}$, a spike event is counted and the state variable\n",
    "$u$ is set back to $u_{\\text{reset}}$ by the reset rule. \n",
    "\n",
    "<!-- dom:FIGURE:[figures/LIF_dynamics.png, height=200, alt=\"Mechanistic analogon of the stochastic LIF neuron\", frac=0.8] Mechanistic analogon of the stochastic LIF neuron. <div id=\"fig:LIF_dynamics\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:LIF_dynamics\"></div>\n",
    "\n",
    "<img src=\"figures/LIF_dynamics.png\" height=\"200\" alt=\"Mechanistic analogon of the stochastic LIF neuron\"><p style=\"font-size: 0.9em\"><i>Figure 4: Mechanistic analogon of the stochastic LIF neuron.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "Drawing the analogy, we were able to solve the model qualitatively, just by our intuitive physical insight.\n",
    "However, it is also possible to solve this linear model analytically.\n",
    "\n",
    "**Analytical Solution of the LIF Model.** Equation ([7](#eq:LIF)) together with an initial value condition $u(t = 0) = u_{\\textrm{reset}}$ defines an initial value problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb1c6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:IVP\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\tau\\, \\frac{d}{dt}\\, u(t) &= - (u(t) - u_{\\textrm{reset}}) + R \\, I(t) \\\\\n",
    "        u(t = 0) &= u_{\\textrm{reset}}\n",
    "    \\end{split} \\quad \\textrm{.}\n",
    "    \\label{eq:IVP} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8eaab",
   "metadata": {
    "editable": true
   },
   "source": [
    "The solution to this initial problem yields the subtreshold dynamics of the LIF.\n",
    "In this section, we want to provide a step-by-step analytical solution.\n",
    "First, let us introduce a new variable $\\tilde{u}(t) := u(t) - u_{\\textrm{reset}}$.\n",
    "Notice that Equation ([10](#eq:IVP)) can be expressed in terms of $\\tilde{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2295c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:IVPtilde\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{d}{dt}\\, \\tilde{u}(t) &= - \\frac{1}{\\tau}\\, \\tilde{u}(t) + \\frac{R}{\\tau} \\, I(t) \\\\\n",
    "        \\tilde{u}(t = 0) &= 0\n",
    "    \\end{split} \\quad \\textrm{.}\n",
    "    \\label{eq:IVPtilde} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382da8d1",
   "metadata": {
    "editable": true
   },
   "source": [
    "The right hand side of Equation ([11](#eq:IVPtilde)) is Lipschitz in $\\tilde{u}$.\n",
    "Moreover, if $I(t)$ is continous in $t$, the ODE is so too.\n",
    "In that case, the Picard-Lindel\\\"of theorem states that the initial value problem in Equation ([11](#eq:IVPtilde)) has one unique solution on $t \\in (0,\\, T)$.\n",
    "Thus, we know that a unique solution exists and we just have to find it.\n",
    "Consequently, since the solution is unique, that also means that as soon we have found one solution, **no matter how**, we know that it is the only one.\n",
    "\n",
    "Equation ([11](#eq:IVPtilde)) describes an **inhomogeneous linear ODE of order $1$**.\n",
    "To solve this inhomogeneous problem, we need to solve the associated homogeneous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2aea96",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:IVPhom\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tilde{u}_{\\textrm{h}}^{\\prime}(t) = -\\frac{1}{\\tau}\\, \\tilde{u}_{\\textrm{h}}(t)\n",
    "    \\label{eq:IVPhom} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d26a3",
   "metadata": {
    "editable": true
   },
   "source": [
    "first.\n",
    "Solving Equation ([12](#eq:IVPhom)), for example by the method of separating the variables, yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69037169",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:IVPhomsolution\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tilde{u}_{\\textrm{h}}(t) = C\\, \\exp{\\left[- \\frac{t}{\\tau} \\right]}, \\quad C \\in \\mathbb{R} \\; \\textrm{.}\n",
    "    \\label{eq:IVPhomsolution} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724801e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "To find the general solution of the inhomogeneous linear problem, we use the **Method of Variation of Parameters**.\n",
    "For that, we assume that the parameter $C$ in Equation ([13](#eq:IVPhomsolution)) becomes a function of time which leaves us with the following Ansatz for a general solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423dd78d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:GenAnsatz\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        C &\\rightarrow C(t) \\\\\n",
    "        \\tilde{u}(t) &= C(t)\\, \\exp{\\left[ - \\frac{t}{\\tau} \\right]}\n",
    "    \\end{split}\n",
    "    \\quad \\textrm{.}\n",
    "    \\label{eq:GenAnsatz} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21748cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "Applying Equation ([14](#eq:GenAnsatz)) to Equation ([11](#eq:IVPtilde)) yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acdae2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C^{\\prime}(t)\\, \\exp{\\left[ - \\frac{t}{\\tau} \\right]} - \\frac{1}{\\tau}\\, C(t)\\, \\exp{\\left[ - \\frac{t}{\\tau} \\right]} = - \\frac{1}{\\tau}\\, C(t)\\, \\exp{\\left[\\frac{t}{\\tau}\\right]} + \\frac{R}{\\tau} \\, I(t)\n",
    "\\label{_auto9} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687516a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "which provides an ODE for $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6c0d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C^{\\prime}(t) = \\frac{R}{\\tau} \\, I(t)\\, \\exp{\\left[ + \\frac{t}{\\tau} \\right]} \\quad \\textrm{,}\n",
    "\\label{_auto10} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0a82e",
   "metadata": {
    "editable": true
   },
   "source": [
    "that can be formally integrated on $(0,\\, T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d1e49",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:VarParSol\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    C(t) = \\frac{R}{\\tau} \\, \\int_{0}^{t} I(t^{\\prime})\\, \\exp{\\left[ + \\frac{t^{\\prime}}{\\tau} \\right]}\\, d t^{\\prime} \\quad \\textrm{.}\n",
    "    \\label{eq:VarParSol} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b101f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Applying Equation ([17](#eq:VarParSol)) to Equation ([14](#eq:GenAnsatz)) gives the solution to the whole Initial Value problem in the subthreshold regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330951cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\tilde{u}(t) = \\frac{R}{\\tau}\\, \\int_{0}^{t} I(t^{\\prime}) \\exp{\\left[\\frac{t^{\\prime}}{\\tau}\\right]} dt^{\\prime} \\, \\exp{\\left[-\\frac{t}{\\tau}\\right]} \\quad \\forall t < t^{f} \\quad \\textrm{.}\n",
    "\\label{_auto11} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70425a17",
   "metadata": {
    "editable": true
   },
   "source": [
    "Backtransformation to the original dynamic variable $u$ yields the true subthreshold LIF trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae02e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u(t) = u_{\\textrm{reset}} + \\frac{R}{\\tau} \\int_{0}^{t} I(t^{\\prime}) \\exp{\\left[\\frac{t^{\\prime}}{\\tau}\\right]} dt^{\\prime} \\, \\exp{\\left[-\\frac{t}{\\tau}\\right]} \\quad \\forall t < t^{f} \\quad \\textrm{.}\n",
    "\\label{_auto12} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f73a0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, $u(t=0) = u_{\\textrm{reset}}$, which satisfies our initial condition.\n",
    "Also note that this solution is valid for all $t < t^{f}$, therefore until the model fires for the first time.\n",
    "Because of the reset rule, note that this solution is valid between every two $t_{i}^{f} < t < t_{i+1}^{f}$ consecutive firing times $t^{f}_{i}$ and $t^{f}_{i+1}$.\n",
    "However, for each Inter Spike Interval, the source term $I(t) = I^{i}(t)$ changes.\n",
    "\n",
    "**IF Model driven by spike train inputs.** Let us assume a concrete neuron model with $N_{\\textrm{syn}}$ input synapses.\n",
    "We want to know the neuron's membrane voltage $u(t)$ between two firing times $t \\in (t^{f}_{i},\\, t^{f}_{i + 1})$ such that we do not have to account for the reset rule.\n",
    "Each of the synapses has a coupling strength $w_{j}$ associated with it. \n",
    "At each synapse, a spike-train $I_{j}(t)$ arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b234c4c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    I_{j}(t) = \\sum_{m \\in \\mathcal{F}_{j}} \\delta(t - t_{m}^{f, j}) \\quad \\textrm{.}\n",
    "\\label{_auto13} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fde4c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\mathcal{F}_{j}$ denotes the spike time index set of synapse $j$.\n",
    "In total, we see that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a154e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto14\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    I(t) = \\sum_{j = 1}^{N_{\\textrm{syn}}} w_{j} \\, \\sum_{m \\in \\mathcal{F}_{j}} \\delta(t - t_{m}^{f, j})\\quad \\textrm{.}\n",
    "\\label{_auto14} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ee36a",
   "metadata": {
    "editable": true
   },
   "source": [
    "Thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d5e0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto15\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     u(t) = u_{\\textrm{reset}} + \\frac{R}{\\tau} \\int_{t_{i}^{f}}^{t} \\sum_{j = 1}^{N_{\\textrm{syn}}} w_{j} \\, \\sum_{m \\in \\mathcal{F}_{j}} \\delta(t^{\\prime} - t_{m}^{f, j}) \\exp{\\left[\\frac{t^{\\prime}}{\\tau}\\right]} dt^{\\prime} \\, \\exp{\\left[-\\frac{t}{\\tau}\\right]} \n",
    "\\label{_auto15} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdc913",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto16\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "     = u_{\\textrm{reset}} + \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} w_{j} \\, \\sum_{m \\in \\mathcal{F}_{j}} \\, \\int_{t_{i}^{f}}^{t} \\delta(t^{\\prime} - t_{m}^{f, j}) \\exp{\\left[- \\frac{t - t^{\\prime}}{\\tau}\\right]} dt^{\\prime} \\quad \\textrm{.}\n",
    "\\label{_auto16} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c6541",
   "metadata": {
    "editable": true
   },
   "source": [
    "Due to the Dirac $\\delta$-function, the integral behaves as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca45ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto17\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\int_{t_{i}^{f}}^{t} \\, \\delta(t^{\\prime} - t^{f, j}_{m})\\, \\exp{\\left[- \\frac{t - t^{\\prime}}{\\tau} \\right]}\\, d t^{\\prime} = \\begin{cases}\n",
    "        \\exp{\\left[ - \\frac{t - t^{f, j}_{m}}{\\tau} \\right]} \\quad \\Leftrightarrow \\quad t^{f, j}_{m} \\in (t^{f}_{i},\\, t) \\\\\n",
    "        0 \\quad \\textrm{else}\n",
    "    \\end{cases} \\quad \\textrm{.}\n",
    "\\label{_auto17} \\tag{24}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d097e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Therefore, only the contributions with firing times lying within the interval $(t_{i}^{f},\\, t)$ will survive.\n",
    "Consequently, the antiderivative on $(t_{i}^{f},\\, t_{i+1}^{f})$ reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4dda3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto18\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     u(t) = u_{\\textrm{reset}} + \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} w_{j} \\, \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{i}^{f}, t_{i + 1}^{f})}} \\exp{\\left[ - \\frac{t - t_{m}^{f, j}}{\\tau}\\right]}\\, \\Theta(t - t_{m}^{f, j}) \\quad \\textrm{,}\n",
    "\\label{_auto18} \\tag{25}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f900c",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\left. \\mathcal{F}_{j} \\right|_{(t_{i}^{f}, t_{i+1}^{f})}$ denoting the respective index set restricted to the interval $(t_{i}^{f}, t_{i + 1}^{f})$.\n",
    "This index set contains all the indices of spikes that enter synapse $j$ in the respective time interval.\n",
    "$\\Theta(\\cdot)$ is the Heavyside step function.\n",
    "\n",
    "**Interpretation of the Solution as an ANN.** Let $T \\in \\mathbb{R}$ be a reasonable time intervall of the order of the model's mean spiking period.\n",
    "We are now interested in a time window $[t_{0}, t_{0} + T]$ shortly after a spike has just occured $t_{0} = t_{i}^{f} + \\epsilon$, $\\epsilon << 1$ such that no further spike will occur during that period and we can assume subthreshold dynamics.\n",
    "Then, the voltage at $t_{0} + T$ can be calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093848b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ConvEq\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u(t_{0} + T) = u(T) = u_{\\textrm{reset}} + \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} w_{j} \\, \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{0}, T)}} \\exp{\\left[-\\frac{T - t_{m}^{f, j}}{\\tau}\\right]}\\, \\Theta(T - t_{m}^{f, j})\n",
    "    \\label{eq:ConvEq} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7d1f0",
   "metadata": {
    "editable": true
   },
   "source": [
    "under the neglection of the fire and reset rule.\n",
    "Thus, to check whether the neuron will spike at $t_{0} + T$, we have to check whether $u(t_{0} + T) > u_{\\textrm{thresh}} \\Leftrightarrow u(t_{0} + T) - u_{\\textrm{thresh}} > 0$.\n",
    "\n",
    "Let us introduce a time discretization $\\{t_{i}\\}_{i \\in \\mathcal{I}}$ of bin size $\\Delta t$ on $[t_{0}, t_{0} + T]$ and denote $\\sum_{m \\in \\left. \\mathcal{F} \\right|_{(t_{0},\\, T)}}\\delta_{t_{m}^{f, j}, t_{i}} = x_{ji} \\in \\mathbb{R}^{N_{j}} \\otimes \\mathbb{R}^{T / \\Delta t}$ the input tensor of binary spike data at all of the synapses.\n",
    "Then, we can express $\\Theta(T - t^{f, j}_{m})$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868958db",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:tautology\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{0},\\, T)}} \\, \\Theta(T - t^{f, j}_{m}) = \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{0},\\, T)}}\\, \\sum_{i = 1}^{T / \\Delta t} \\, \\delta_{t_{m}^{f, j}, t_{i}} \n",
    "    = \\sum_{i = 1}^{T / \\Delta t} \\, x_{ji} \\quad \\textrm{.}\n",
    "    \\label{eq:tautology} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739294d",
   "metadata": {
    "editable": true
   },
   "source": [
    "With Equation ([27](#eq:tautology)), Equation ([26](#eq:ConvEq)) becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdc0d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:DiscreteConv\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        u(t_{0} + T) &=& \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} \\, \\sum_{i = 0}^{T / \\Delta t} \\,w_{j}\\, \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{0},\\, T)}}\\, \\delta_{t_{m}^{f, j}, t_{i}} \\, \\exp{\\left[-\\frac{T - t_{m}^{f, j}}{\\tau}\\right]} + u_{\\textrm{reset}} \\\\\n",
    "        &=& \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} \\, \\sum_{i = 0}^{T / \\Delta t} \\, w_{j}\\, \\sum_{m \\in \\left. \\mathcal{F}_{j} \\right|_{(t_{0},\\, T)}}\\, \\delta_{t_{m}^{f, j}, t_{i}} \\,\\exp{\\left[-\\frac{T - t_{i}}{\\tau}\\right]} + u_{\\textrm{reset}} \\\\\n",
    "        &=& \\frac{R}{\\tau} \\, \\sum_{j = 1}^{N_{\\textrm{syn}}} \\, \\sum_{i = 0}^{T / \\Delta t} \\, w_{j}\\, \\exp{\\left[-\\frac{T - t_{i}}{\\tau}\\right]}\\, x_{ji} + u_{\\textrm{reset}} \\quad \\textrm{.}\n",
    "    \\end{split}\n",
    "    \\label{eq:DiscreteConv} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d720f",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can interpret Equation ([28](#eq:DiscreteConv)) as a 2D discrete convolution of the input tensor $x_{ji}$ across the time domain with an exponential filter $\\exp{\\left[-\\frac{T - t_{i}}{\\tau}\\right]}$ and the synapse domain with a step filter $w_{j}$.\n",
    "Thus, we identify the 2D filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addb3cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto19\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    K^{ji} = \\frac{R}{\\tau}\\, w_{j}\\, \\exp{\\left[-\\frac{T - t_{i}}{\\tau}\\right]} \\quad \\textrm{.}\n",
    "\\label{_auto19} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e08f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using Einstein summation, we can express Equation ([28](#eq:DiscreteConv)) more compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93860d17",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto20\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u(t_{0} + T) = K^{ji} x_{ji} + u_{\\textrm{reset}} \\quad \\textrm{.}\n",
    "\\label{_auto20} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082f4f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can interpret this summation as tensor contraction or a generalised dot product. \n",
    "Flattening both, $x_{ji}$ and $K^{ji}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1f205",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{x} &= \\left( \\mathbf{x}_{0 i},\\, \\mathbf{x}_{1 i},\\, \\dots,\\, \\mathbf{x}_{N_{\\textrm{syn}} i} \\right) \\\\\n",
    "    \\mathbf{K} &= \\left( \\mathbf{K}_{0 i},\\, \\mathbf{K}_{1 i},\\, \\dots,\\, \\mathbf{K}_{N_{\\textrm{syn}} i} \\right) \\quad \\textrm{,}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08dab4",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can express both convolutions in familiar matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c3e96",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto21\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u(t_{0} + T) = \\mathbf{K}^{T} \\mathbf{x} + u_{\\textrm{reset}} \\quad \\textrm{.}\n",
    "\\label{_auto21} \\tag{31}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f3cc3",
   "metadata": {
    "editable": true
   },
   "source": [
    "Consequently, we can treat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670158a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto22\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        P(t) &= \\textrm{ReLU}(u(t)-u_{\\textrm{thresh}}) \\\\\n",
    "        &= \\textrm{ReLU} \\left[\\mathbf{K}^{T} \\mathbf{x} + u_{\\textrm{reset}} - u_{\\textrm{thresh}}\\right]\n",
    "    \\end{split}\n",
    "\\label{_auto22} \\tag{32}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8528661",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t = t_{0} + T$ as a proxy for the time that has past since the last spike of the LIF.\n",
    "Formally, this approximation of our LIF resembles the equation of an Perceptron with input $\\textbf{x}$, weight matrix $\\textbf{K}$, bias $u_{\\textrm{reset}} - u_{\\textrm{thresh}}$ and a ReLU activation function.\n",
    "However, note that a temporal convolution hides behind this simple expression which is why this particular model can be considered the elementary case of a **temporal convolutionel neural network (TCNN)**.\n",
    "\n",
    "In summary, we showed that the LIF model is the edge case in which the dynamic model can exactly be represented by a recurrent TCNN with one artificial neuron. \n",
    "However, as we will see, this does not generalise to more complex integrate and fire models.\n",
    "Already when we want to implement adaptation, we need a more complex network of artificial neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372251e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From Spiking Neurons to Deep Neural Networks\n",
    "<!-- Vemund responsible for the upcoming text -->\n",
    "\n",
    "When attempting to model the functionality of a biological neuron, we can approach it from various angles and with varying degrees of complexity. \n",
    "On one hand, we have the highly detailed and biophysically accurate models like the Hodgkin-Huxley model. \n",
    "On the other hand, we have simpler, more abstract models, such as artificial neurons.\n",
    "\n",
    "This raises an important question: Which model should we opt for? The choice is highly dependent on the specific goal of our study. \n",
    "For instance, if we are interested in understanding the complex biophysical intricacies of a neuron, a detailed model might be preferred. \n",
    "However, if our focus is on large-scale simulations of neuronal networks, an abstract model would be more suitable due to its computational efficiency.\n",
    "\n",
    "Why would we choose a more abstract model over a \"more accurate\" one? \n",
    "The answer lies in computational cost. \n",
    "Higher levels of abstraction allow us to ignore nonessential details, reducing computational time and enabling us to scale our simulations. \n",
    "This scalability allows us to model and examine networks consisting of billions of neuron connections. \n",
    "The advent of deep learning over the past decade has shown us the potential of these abstract models. \n",
    "Deep learning has tackled and solved numerous complex challenges, such as Chess and Go games, protein folding, image recognition, detection, and captioning, Language Learning Models (LLMs), image generation, and more. \n",
    "This success underscores the value and strength of simple artificial neurons and their ability to form complex behaviors and functions when networked.\n",
    "\n",
    "However, it's also valid to question the necessity of detailed neuron models if high-level artificial neurons can encapsulate the interesting aspects of a biological neuron. \n",
    "Despite the advancements, artificial neural networks still face several challenges, such as [catastrophic forgetting](https://linkinghub.elsevier.com/retrieve/pii/S0079742108605368). \n",
    "This raises the possibility that some of these issues might be addressed by incorporating more intricate details from biophysically accurate neuron models into artificial neurons.\n",
    "\n",
    "In this section, we aim to examine the bridge between intricate neuron models and simple artificial ones. \n",
    "At the same time we will introduce PyTorch and some core machine learning methods. \n",
    "To do so, we will explore how a network of simple artificial neurons can emulate the behavior of complex biological ones. \n",
    "For this purpose, we will draw inspiration from the recent research paper by [Beniaguev2021](https://linkinghub.elsevier.com/retrieve/pii/S0896627321005018), where they demonstrate that complex, biophysically detailed neurons can be approximated by deep neural networks (many simple artificial neurons). \n",
    "In this notebook we will reproduce their initial first experiment where they approximate a LIF-neuron with a single-layer (many-to-one) artificial neural network. \n",
    "In this case it turns out we can actually analytically solve how the learned weights should look like - which we showed in the above sections. \n",
    "This provides a neat test whether for whether our learned model behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69cd6b0",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from methods import poisson_neurons, spike_trains_to_binary, lif_integrate, lif_direct, lif_with_adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a194f",
   "metadata": {
    "editable": true
   },
   "source": [
    "To start, we simulate a set of incoming spikes sampled from a poisson process. \n",
    "Then we create the output response using a LIF. \n",
    "This will define the input/output relation we will try to approximate using an ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ffa729",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# input parameter\n",
    "n = 35\n",
    "N_exc = 4*n  # number of excitatory synapses\n",
    "N_inh = 1*n  # number of inhibitory synapses\n",
    "fr_exc = 1.4  # excitatory firing rate in Hz\n",
    "fr_inh = 1.3  # inhibitory firing rate in Hz\n",
    "\n",
    "# model parameter\n",
    "w_exc = 2.0  # excitatory synaptic weight\n",
    "w_inh = -2.0  # inhibitory synaptic weight\n",
    "u_thresh = -55.0 # mV\n",
    "tau = 20.0  # membrane time constant in ms\n",
    "ur = -70.0  # resting membrane potential in mV\n",
    "\n",
    "# simulation parameter\n",
    "T = 60000 # simulation time in ms\n",
    "dt = 1.0 # time step in ms\n",
    "\n",
    "# Generate excitatory and inhibitory Poisson spike trains\n",
    "exc_spike_times = poisson_neurons(N_exc, fr_exc, duration=T)\n",
    "inh_spike_times = poisson_neurons(N_inh, fr_inh, duration=T)\n",
    "\n",
    "# Convert spike times to binary spike trains\n",
    "exc_spike_trains = spike_trains_to_binary(exc_spike_times, duration=T, bin_size=dt)\n",
    "inh_spike_trains = spike_trains_to_binary(inh_spike_times, duration=T, bin_size=dt)\n",
    "# stack the two spike trains\n",
    "spike_trains = np.vstack((exc_spike_trains, inh_spike_trains))\n",
    "\n",
    "# Simulate the LIF neuron\n",
    "#us, out_spike_train = lif_integrate(exc_spike_trains, inh_spike_trains, tau=tau, dt=dt, ur=ur, u_thresh=u_thresh, w_exc=w_exc, w_inh=w_inh)\n",
    "us, out_spike_train = lif_direct(exc_spike_trains, inh_spike_trains, tau=tau, ur=ur, u_thresh=u_thresh, w_exc=w_exc, w_inh=w_inh)\n",
    "#us, out_spike_train = lif_with_adaptation(exc_spike_trains, inh_spike_trains, dt=dt, T=T, u_rest=ur, u_thresh=u_thresh, tau_m=tau, tau_w=200, a=35, b=1e-9, R=1, w_exc=w_exc, w_inh=w_inh)\n",
    "\n",
    "# Calculate the output firing rate\n",
    "output_firing_rate = np.sum(out_spike_train) / (T / 1000)  # spikes per second\n",
    "print(f\"LIF Firing Rate = {output_firing_rate:.4f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c7700",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Show (excitatory and inhibitory) input spikes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afb5756",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "color = ['red']*N_exc + ['blue']*N_inh\n",
    "# only plot spike event for the times smaller than chosen amount of ms\n",
    "spike_times = exc_spike_times + inh_spike_times\n",
    "spike_times = [spike_time[spike_time < 6000] for spike_time in spike_times]\n",
    "ax.eventplot(spike_times, color=color)\n",
    "# create blue and red legend for excitatory and inhibitory neurons\n",
    "ax.legend(['Excitatory', 'Inhibitory'])\n",
    "leg = ax.get_legend()\n",
    "leg.legend_handles[0].set_color('red')\n",
    "leg.legend_handles[1].set_color('blue')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Neuron index')\n",
    "ax.set_title('Raster plot of excitatory and inhibitory neurons')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8136ea3",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Show LIF-neuron output based on the given input spikes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8698dd7",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tmax = 6000\n",
    "time = np.arange(0, tmax, dt)\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax[0].plot(time, us[:len(time)])\n",
    "ax[0].set_ylabel('Membrane potential (mV)')\n",
    "\n",
    "ax[1].plot(time, out_spike_train[:len(time)])\n",
    "ax[1].set_xlabel('Time (ms)')\n",
    "ax[1].set_ylabel('Spike train')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d2be9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Constructing a Deep Learning Model for Neural Simulation\n",
    "\n",
    "Next, we'll construct a deep learning model designed to approximate the behavior of the Leaky Integrate-and-Fire (LIF) neuron discussed earlier. \n",
    "According to the findings in Beniaguev2021, and our initial analytical derivation, a single-layer temporal convolutional model is sufficient for this task. \n",
    "To make the training process more efficient, we'll implement several strategic methods. \n",
    "These strategies include:\n",
    "\n",
    "1. Utilizing the [Adam optimizer](https://arxiv.org/abs/1412.6980), an advanced gradient descent training algorithm. \n",
    "Adam incorporates techniques such as mini-batching, momentum, and an adaptive learning rate to optimize the training process.\n",
    "2. Employing a learning rate scheduler to gradually reduce the learning rate during training. \n",
    "This (heuristically) helps the model to converge more accurately towards a local minimum by incrementally decreasing the step size of each gradient update.\n",
    "3. Applying a dynamic weighting scheme in the classification loss that takes into account the frequency of different classes. \n",
    "This ensures that the network assigns equal importance to all class types, even when the dataset is skewed towards one class over others.\n",
    "4. Standardizing the input/output (I/O) data. \n",
    "In our case, we standardize the target voltage only. \n",
    "Neural network weights are typically initialized with the assumption that input data is centered and scaled appropriately. \n",
    "If this isn't the case, training can become tediously slow.\n",
    "\n",
    "To build our deep learning model using the PyTorch library, we'll follow these steps:\n",
    "\n",
    "1. Create a class that inherits from the \"torch.nn.Module\". \n",
    "This forms the skeleton of our neural network model.\n",
    "2. Initialize your network by specifying the components that will make up your model architecture. \n",
    "This is where we define the layers and their parameters.\n",
    "3. Define a `forward()` method. \n",
    "This method outlines the computational flow of the network, connecting the previously defined layers and specifying their interactions. \n",
    "This function essentially overrides the `__call__()` method of the class, allowing us to pass input data through the model.\n",
    "4. Define a loss function, or `loss_fn()`. \n",
    "This function quantifies the difference between the model's predictions and the actual targets, providing a measure that can be minimized during training. \n",
    "Although it's not required to include this function within the model class, it's a convenient place to put it for organizational purposes.\n",
    "\n",
    "Luckily, the PyTorch skeleton follows the style of our previous MLP class, except now we will use PyTorch's built in autodiff rather than our Variable class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d3d9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Fill in** the blanks `___` in the `__init__`, `forward` and `loss_fn` methods of the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f11897a",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DeepLIF(torch.nn.Module):\n",
    "    def __init__(self, n_inputs=175, n_time_bins=80, lr=1e-2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            n_inputs, int: number of input excitatory + inhibitory neurons\n",
    "            n_time_bins, int: number of time bins in the simulation (assume dt=1ms)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_time_bins = n_time_bins\n",
    "\n",
    "        # connect all inputs at all time bins to a single output neuron, i.e. a fully connected layer with \n",
    "        # shape (n_inputs * n_time_bins, 1), and a bias term. Hint! Check out the torch.nn.Linear module\n",
    "        # documentation.\n",
    "        self.W = ___ # <---- FILL IN\n",
    "        # initialize the Adam optimizer (see torch.optim.Adam documentation) with learning rate lr\n",
    "        self.optimizer = ___ # <---- FILL IN\n",
    "        # initialize the learning rate scheduler (see torch.optim.lr_scheduler.StepLR documentation) with\n",
    "        # step_size=20 and gamma=0.9\n",
    "        self.scheduler = ___ # <---- FILL IN\n",
    "    \n",
    "    def forward(self, input_spikes, sigmoid=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_spikes, torch.Tensor: input tensor of shape (batch_size, n_inputs, n_time_bins)\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # if input_spikes are not a tensor, convert to tensor\n",
    "        if not isinstance(input_spikes, torch.Tensor):\n",
    "            input_spikes = torch.tensor(input_spikes, dtype=torch.float32)\n",
    "        # flatten the input tensor to shape (batch_size, n_inputs * n_time_bins)\n",
    "        input_spikes = input_spikes.reshape(input_spikes.shape[0], -1)\n",
    "        pred_voltages = ___ # <---- FILL IN\n",
    "        if sigmoid:\n",
    "            pred_spikes = ___ # <---- FILL IN\n",
    "            return pred_spikes\n",
    "        return pred_voltages\n",
    "    \n",
    "    def loss_fn(self, input_spikes, true_spikes, true_voltages, lambda_spikes=0.1, lambda_voltages=1.0):\n",
    "        \"\"\"\n",
    "        Loss function for both spike and voltage loss.\n",
    "\n",
    "        Parameters:\n",
    "            input_spikes, torch.Tensor: input tensor of shape (batch_size, n_inputs, n_time_bins)\n",
    "            true_spikes, torch.Tensor: true spike tensor of shape (batch_size, 1)\n",
    "            true_voltages, torch.Tensor: true voltage tensor of shape (batch_size, 1)\n",
    "        Returns:\n",
    "            torch.Tensor: loss tensor of shape (1,)\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        pred_voltages = self.forward(input_spikes, sigmoid=False)\n",
    "        pred_spikes = torch.sigmoid(pred_voltages)\n",
    "        # calculate loss\n",
    "        spike_loss, voltage_loss = 0, 0\n",
    "        # whether to include spike loss\n",
    "        if true_spikes is not None:\n",
    "            # must change shape of true_spikes\n",
    "            true_spikes = torch.tensor(true_spikes, dtype=torch.float32) if not isinstance(true_spikes, torch.Tensor) else true_spikes\n",
    "            # Calculate class frequencies\n",
    "            unique, counts = true_spikes.unique(return_counts=True)\n",
    "            class_weights = 1. / counts.float()\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "            # Create a weight tensor based on the class frequencies\n",
    "            weight = torch.zeros_like(true_spikes)\n",
    "            for i, u in enumerate(unique):\n",
    "                weight[true_spikes == u] = class_weights[i]\n",
    "\n",
    "            # Calculated weighted binary cross entropy loss\n",
    "            spike_loss = ___ # <---- FILL IN\n",
    "        # whether to include voltage loss\n",
    "        if true_voltages is not None:\n",
    "            true_voltages = torch.tensor(true_voltages, dtype=torch.float32) if not isinstance(true_voltages, torch.Tensor) else true_voltages\n",
    "            # Calculate the mean squared error loss\n",
    "            voltage_loss = ___ # <---- FILL IN\n",
    "        return lambda_spikes * spike_loss + lambda_voltages * voltage_loss\n",
    "    \n",
    "    def train_step(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            kwargs, dict: keyword arguments for loss_fn\n",
    "        Returns:\n",
    "            torch.Tensor: loss tensor of shape (1,)\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(**kwargs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6178a",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Instantiate the model and look at initial (random) connectivity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1b7d78",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initialise model\n",
    "model = DeepLIF()\n",
    "loss_history = []\n",
    "# standardize (z-score) the target voltages\n",
    "standardized_us = (us - np.mean(us)) / np.std(us) #us - np.mean(us) \n",
    "# save initial weight profile\n",
    "initial_connectivity = model.W.weight.detach().clone().numpy().reshape(model.n_inputs, model.n_time_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a425b8",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot weight profile\n",
    "plt.imshow(initial_connectivity, origin='lower', aspect='auto', cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time bin [ms]')\n",
    "plt.ylabel('Input neuron index')\n",
    "plt.title('Initial Weight profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6fdaaf",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DeepLIF(torch.nn.Module):\n",
    "    def __init__(self, n_inputs=175, n_time_bins=80, lr=1e-2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            n_inputs, int: number of input excitatory + inhibitory neurons\n",
    "            n_time_bins, int: number of time bins in the simulation (assume dt=1ms)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_time_bins = n_time_bins\n",
    "\n",
    "        # connect all inputs at all time bins to a single output neuron, i.e. a fully connected layer with \n",
    "        # shape (n_inputs * n_time_bins, 1), and a bias term. Hint! Check out the torch.nn.Linear module\n",
    "        # documentation.\n",
    "        self.W = torch.nn.Linear(n_inputs * n_time_bins, 1, bias=True)\n",
    "        # initialize the Adam optimizer (see torch.optim.Adam documentation) with learning rate lr\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        # initialize the learning rate scheduler (see torch.optim.lr_scheduler.StepLR documentation) with\n",
    "        # step_size=20 and gamma=0.9\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.9)\n",
    "    \n",
    "    def forward(self, input_spikes, sigmoid=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_spikes, torch.Tensor: input tensor of shape (batch_size, n_inputs, n_time_bins)\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # if input_spikes are not a tensor, convert to tensor\n",
    "        if not isinstance(input_spikes, torch.Tensor):\n",
    "            input_spikes = torch.tensor(input_spikes, dtype=torch.float32)\n",
    "        # flatten the input tensor to shape (batch_size, n_inputs * n_time_bins)\n",
    "        input_spikes = input_spikes.reshape(input_spikes.shape[0], -1)\n",
    "        pred_voltages = self.W(input_spikes)\n",
    "        if sigmoid:\n",
    "            pred_spikes = torch.sigmoid(pred_voltages)\n",
    "            return pred_spikes\n",
    "        return pred_voltages\n",
    "    \n",
    "    def loss_fn(self, input_spikes, true_spikes, true_voltages, lambda_spikes=0.1, lambda_voltages=1.0):\n",
    "        \"\"\"\n",
    "        Loss function for both spike and voltage loss.\n",
    "\n",
    "        Parameters:\n",
    "            input_spikes, torch.Tensor: input tensor of shape (batch_size, n_inputs, n_time_bins)\n",
    "            true_spikes, torch.Tensor: true spike tensor of shape (batch_size, 1)\n",
    "            true_voltages, torch.Tensor: true voltage tensor of shape (batch_size, 1)\n",
    "        Returns:\n",
    "            torch.Tensor: loss tensor of shape (1,)\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        pred_voltages = self.forward(input_spikes, sigmoid=False)\n",
    "        pred_spikes = torch.sigmoid(pred_voltages)\n",
    "        # calculate loss\n",
    "        spike_loss, voltage_loss = 0, 0\n",
    "        # whether to include spike loss\n",
    "        if true_spikes is not None:\n",
    "            # must change shape of true_spikes\n",
    "            true_spikes = torch.tensor(true_spikes, dtype=torch.float32) if not isinstance(true_spikes, torch.Tensor) else true_spikes\n",
    "            # Calculate class frequencies\n",
    "            unique, counts = true_spikes.unique(return_counts=True)\n",
    "            class_weights = 1. / counts.float()\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "            # Create a weight tensor based on the class frequencies\n",
    "            weight = torch.zeros_like(true_spikes)\n",
    "            for i, u in enumerate(unique):\n",
    "                weight[true_spikes == u] = class_weights[i]\n",
    "\n",
    "            # Use the weight tensor in the binary cross entropy loss\n",
    "            spike_loss = torch.nn.functional.binary_cross_entropy(pred_spikes, true_spikes, weight=weight)\n",
    "        # whether to include voltage loss\n",
    "        if true_voltages is not None:\n",
    "            true_voltages = torch.tensor(true_voltages, dtype=torch.float32) if not isinstance(true_voltages, torch.Tensor) else true_voltages\n",
    "            voltage_loss = torch.nn.functional.mse_loss(true_voltages, pred_voltages)\n",
    "        return lambda_spikes * spike_loss + lambda_voltages * voltage_loss\n",
    "    \n",
    "    def train_step(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            kwargs, dict: keyword arguments for loss_fn\n",
    "        Returns:\n",
    "            torch.Tensor: loss tensor of shape (1,)\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(**kwargs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f172b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Define data sampling strategy by picking out random 80ms spike-train windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e0106",
   "metadata": {
    "editable": true
   },
   "source": [
    "Complete the code below to sample a data batch to be used for training. \n",
    "The method should slice out `window_size` portions from the `full n_time_bins` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8f6fb5",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_data_batch(spike_trains, standardized_us, out_spike_train, window_size = 80, batch_size=128, sequential=False):\n",
    "    \"\"\"\n",
    "    Sample a batch of data from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains, np.ndarray: spike trains of shape (n_inputs, n_time_bins)\n",
    "        standardized_us, np.ndarray: standardized (z-scored) membrane potentials of shape (n_time_bins,)\n",
    "        out_spike_train, np.ndarray: output spike train of shape (n_time_bins,)\n",
    "        window_size, int: size of window to sample\n",
    "        batch_size, int: number of samples to take\n",
    "        sequential, bool: whether to sample sequentially or randomly\n",
    "    Returns:\n",
    "        x, torch.Tensor: input tensor of shape (batch_size, n_inputs, window_size)\n",
    "        y, torch.Tensor: output tensor of shape (batch_size, 1)\n",
    "        u, torch.Tensor: membrane potential tensor of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    # sample start indices\n",
    "    if sequential:\n",
    "        start_indices = np.arange(0, batch_size, 1)\n",
    "    else:\n",
    "        start_indices = np.random.randint(0, spike_trains.shape[1]-window_size-1, batch_size)\n",
    "    # get slices of spike trains and membrane potentials for each of the sampled start indices\n",
    "\n",
    "    # \n",
    "    # TODO: Create mini-batches of spike trains and membrane potentials by slicing\n",
    "    # \n",
    "\n",
    "    # convert to torch tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    u = torch.from_numpy(u).float()\n",
    "    return x, y, u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47eaa7c8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_data_batch(spike_trains, standardized_us, out_spike_train, window_size = 80, batch_size=128, sequential=False):\n",
    "    \"\"\"\n",
    "    Sample a batch of data from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains, np.ndarray: spike trains of shape (n_inputs, n_time_bins)\n",
    "        standardized_us, np.ndarray: standardized (z-scored) membrane potentials of shape (n_time_bins,)\n",
    "        out_spike_train, np.ndarray: output spike train of shape (n_time_bins,)\n",
    "        window_size, int: size of window to sample\n",
    "        batch_size, int: number of samples to take\n",
    "        sequential, bool: whether to sample sequentially or randomly\n",
    "    Returns:\n",
    "        x, torch.Tensor: input tensor of shape (batch_size, n_inputs, window_size)\n",
    "        y, torch.Tensor: output tensor of shape (batch_size, 1)\n",
    "        u, torch.Tensor: membrane potential tensor of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    if sequential:\n",
    "        start_indices = np.arange(0, batch_size, 1)\n",
    "    else:\n",
    "        start_indices = np.random.randint(0, spike_trains.shape[1]-window_size-1, batch_size)\n",
    "    # get slices of spike trains and membrane potentials for each of the sampled start indices\n",
    "    x = np.array([spike_trains[:, start_idx:start_idx+window_size] for start_idx in start_indices])\n",
    "    y = np.array([out_spike_train[start_idx+window_size:start_idx+window_size+1] for start_idx in start_indices])\n",
    "    u = np.array([standardized_us[start_idx+window_size:start_idx+window_size+1] for start_idx in start_indices])\n",
    "    # convert to torch tensors\n",
    "    x = torch.from_numpy(x).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    u = torch.from_numpy(u).float()\n",
    "    return x, y, u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898113da",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Train the model and show the training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3f97a",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Fill in** the blanks `___` in the code below to complete the training loop of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a181f0eb",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# train model\n",
    "n_epochs = 250\n",
    "batch_size = 64\n",
    "window_size = 80\n",
    "total_batches = int(T / dt) * n_epochs / (batch_size * window_size)\n",
    "pbar = tqdm.tqdm(total=total_batches, desc='Training progress')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train on one batch of data\n",
    "    pbar.update()\n",
    "    for i in range(int(T / dt) // (batch_size * window_size)):\n",
    "        x, y, u = ___ # <---- FILL IN\n",
    "        loss = ___ # <---- FILL IN\n",
    "        loss_history.append(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            pbar.set_postfix({'loss': loss.item(), 'Epoch': epoch + 1})\n",
    "        pbar.update()\n",
    "    model.scheduler.step()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875b1911",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(loss_history)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ed8129",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# train model\n",
    "n_epochs = 250\n",
    "batch_size = 64\n",
    "window_size = 80\n",
    "total_batches = int(T / dt) * n_epochs / (batch_size * window_size)\n",
    "pbar = tqdm.tqdm(total=total_batches, desc='Training progress')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train on one batch of data\n",
    "    pbar.update()\n",
    "    for i in range(int(T / dt) // (batch_size * window_size)):\n",
    "        x, y, u = sample_data_batch(spike_trains, standardized_us, out_spike_train, window_size = window_size ,batch_size=batch_size)\n",
    "        loss = model.train_step(input_spikes=x, true_spikes=y, true_voltages=u)\n",
    "        loss_history.append(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            pbar.set_postfix({'loss': loss.item(), 'Epoch': epoch + 1})\n",
    "        pbar.update()\n",
    "    model.scheduler.step()\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d03b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "**Show the learned connectivity, and compare to mathematically derived \"ground truth\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b37b8c",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get weight profile\n",
    "learned_connectivity = model.W.weight.detach().clone().numpy().reshape(model.n_inputs, model.n_time_bins)*np.std(us)\n",
    "# smooth connectivity in time using a 1d gaussian filter from scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "smoothed_connectivity = gaussian_filter1d(learned_connectivity, sigma=5, axis=1)#, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc908a7",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(25, 4))\n",
    "# --- plot weight profile ---\n",
    "ax[0].imshow(initial_connectivity, origin='lower', aspect='auto', cmap='jet')\n",
    "ax[0].set_xlabel('Time bin [ms]')\n",
    "ax[0].set_ylabel('Input neuron index')\n",
    "ax[0].set_title('Initial Weight profile')\n",
    "# add colorbar\n",
    "cbar = ax[0].figure.colorbar(ax[0].images[0], ax=ax[0])\n",
    "\n",
    "# --- plot initial weight profile ---\n",
    "ax[1].imshow(learned_connectivity, origin='lower', aspect='auto', cmap='jet')\n",
    "ax[1].set_xlabel('Time bin [ms]')\n",
    "ax[1].set_ylabel('Input neuron index')\n",
    "ax[1].set_title('Learned Weight profile')\n",
    "# add colorbar\n",
    "cbar = ax[1].figure.colorbar(ax[1].images[0], ax=ax[1])\n",
    "\n",
    "# --- plot smoothed weight profile ---\n",
    "ax[2].imshow(smoothed_connectivity, origin='lower', aspect='auto', cmap='jet')\n",
    "ax[2].set_xlabel('Time bin [ms]')\n",
    "ax[2].set_ylabel('Input neuron index')\n",
    "ax[2].set_title('Smoothed Weight profile')\n",
    "# add colorbar\n",
    "cbar = ax[2].figure.colorbar(ax[2].images[0], ax=ax[2])\n",
    "\n",
    "# --- plot \"ground truth\" weight profile ---\n",
    "def K(spike_counts, weight):\n",
    "    # kernel function\n",
    "    t_spike = np.arange(len(spike_counts))\n",
    "    return weight * spike_counts * np.exp(-t_spike[::-1] / tau)\n",
    "\n",
    "ground_truth_connectivity_exc = K(np.ones(model.n_time_bins), w_exc)\n",
    "ground_truth_connectivity_inh = K(np.ones(model.n_time_bins), w_inh)\n",
    "# repeat for each excitatory and inhibitory input neuro (as the weights are identical for each type of input neuron)\n",
    "ground_truth_connectivity_exc = np.tile(ground_truth_connectivity_exc, (N_exc, 1))\n",
    "ground_truth_connectivity_inh = np.tile(ground_truth_connectivity_inh, (N_inh, 1))\n",
    "# stack the ground truth connectivity matrices\n",
    "ground_truth_connectivity = np.vstack((ground_truth_connectivity_exc, ground_truth_connectivity_inh))\n",
    "# plot ground truth connectivity\n",
    "ax[3].imshow(ground_truth_connectivity, origin='lower', aspect='auto', cmap='jet')\n",
    "ax[3].set_xlabel('Time bin [ms]')\n",
    "ax[3].set_ylabel('Input neuron index')\n",
    "ax[3].set_title('\"Ground truth\" Weight profile')\n",
    "# add colorbar\n",
    "cbar = ax[3].figure.colorbar(ax[3].images[0], ax=ax[3])\n",
    "\n",
    "# --- plot mean weight profile ---\n",
    "ax[4].plot(np.mean(learned_connectivity[:N_exc], axis=0), label='Excitatory', color='red')\n",
    "ax[4].plot(np.mean(learned_connectivity[N_exc:], axis=0), label='Inhibitory', color='blue')\n",
    "#ax[4].plot(np.mean(smoothed_connectivity[:N_exc], axis=0), label='Smoothed', color='red', linestyle='--')\n",
    "#ax[4].plot(np.mean(smoothed_connectivity[N_exc:], axis=0), color='blue', linestyle='--')\n",
    "ax[4].plot(np.mean(ground_truth_connectivity[:N_exc], axis=0), color='red', linestyle=':')\n",
    "ax[4].plot(np.mean(ground_truth_connectivity[N_exc:], axis=0), label='Ground truth', color='blue', linestyle=':')\n",
    "#ax[4].plot(np.mean(initial_connectivity[:N_exc], axis=0), color='red', linestyle='-.')\n",
    "#ax[4].plot(np.mean(initial_connectivity[N_exc:], axis=0), label='Initial', color='blue', linestyle='-.')\n",
    "ax[4].set_xlabel('Time bin [ms]')\n",
    "ax[4].set_ylabel('Mean weight')\n",
    "ax[4].set_title('Mean weight profile')\n",
    "ax[4].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df584678",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Check how well the model predicts voltage compared to the true voltage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3c56c1",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get predicted voltage trace\n",
    "x, y, u = sample_data_batch(spike_trains, us, out_spike_train, batch_size=200, sequential=True)\n",
    "predicted_voltages = model(x, sigmoid=False).detach().numpy() * np.std(us) + np.mean(us)\n",
    "# plot the predicted voltage trace against the true voltage trace\n",
    "plt.plot(u[:,0], label='True voltage', color='black')\n",
    "plt.plot(predicted_voltages[:,0], label='Predicted voltage', color='red')\n",
    "plt.xlabel('Time bin [ms]')\n",
    "plt.ylabel('Voltage')\n",
    "plt.title('Voltage trace')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
