{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "381bdbde",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb ReverseAutoDiff.do.txt  -->\n",
    "\n",
    "## Reverse-mode automatic differentiation\n",
    "Deep learning frameworks are built upon the foundation of automatic differentiation (AD). Training deep learning models typically involves gradient-based techniques, with autodiff streamlining the gradient acquisition process, even for large and intricate models. \n",
    "The majority of deep learning frameworks utilize 'reverse-mode autodiff' due to its efficiency and precision.\n",
    "\n",
    "In this module, we will delve into the generalization of the chain rule for AD of any function. This is achieved by understanding that all functions are composed of basic operations such as addition, multiplication, subtraction, and division.\n",
    "\n",
    "AD relies on the fact that all computer programs, no matter how complicated, use a finite set of elementary (unary or binary, e.g. $sin(\\cdot)$, $sqrt(\\cdot)$) operations as defined by the programming language. \n",
    "The value or function computed by the program is simply a composition of these elementary functions. The partial derivatives of the elementary functions are known, and the overall derivatives can be computed using the chain rule.\n",
    "\n",
    "**Notice.**\n",
    "\n",
    "The initial examples and code base are modified from <https://sidsite.com/posts/autodiff/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7ceb6b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6229a41",
   "metadata": {},
   "source": [
    "## Total derivative and gradient\n",
    "The function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$, is totally differentiable at $x\\in\\mathbb{R}^n$ if there is a linear map $df : \\mathbb{R}^n \\to \\mathbb{R}^m$ that satisfies\n",
    "\\begin{align*}\n",
    "    f(x + dx) = f(x) + df_{x}(dx) + \\epsilon(dx),\\; dx\\in\\mathbb{R}^n\n",
    "    \\\\\n",
    "    \\lim_{dx\\to0} \\frac{\\epsilon}{||dx||} = 0\n",
    "\\end{align*}\n",
    "where $df_{x}$ is the total derivative and defined as the best linear approximation at point $x$.\n",
    "\n",
    "The total derivative is also written as $df_x(dx)=Df_x(dx)=J_f(x)\\times dx$ with the Jacobian $J\\in\\mathbb{R}^{m\\times n}$. When the function $f : \\mathbb{R}^n \\to \\mathbb{R}$ maps to one dimension the gradient denoted as $\\nabla f \\in \\mathbb{R}^n$ is the transposed Jacobian $J_f^T$\n",
    "\n",
    "There are two modes of AD, forward and reverse. When $m << n$ reverse mode AD is the most efficient and therefore most used with deep learning.\n",
    "\n",
    "## Chain rule\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$, $g: \\mathbb{R}^k \\to \\mathbb{R}$ and $h: \\mathbb{R}^n \\to \\mathbb{R}^k$ with $x \\in \\mathbb{R}^n$.\n",
    "\n",
    "Let $f(x) = g\\circ h = g(h(x))$. Then, the chain rule gives a nice relation between the total derivatives the Jacobian and the gradient.\n",
    "\n",
    "\\begin{align*}\n",
    "    df(x) = dg \\circ dh = dg(dh(x))\\\\\n",
    "    \\nabla f_x = (J_g \\times J_h)^T\n",
    "\\end{align*}\n",
    "\n",
    "## Composite Functions\n",
    "\n",
    "We can generalize the procedure for computing derivatives if the output is obtained through the evaluation of a composite function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ composed of $ p $ single functions $f_{j+1}: \\mathbb{R}^{k_j} \\to \\mathbb{R}^{k_{j+1}}, k_p=m,k_1=n$ (which is, in fact, how computer codes are represented):\n",
    "\n",
    "$$ f = f_p \\circ f_{p-1} \\circ \\cdots \\circ f_1(x) $$\n",
    "\n",
    "In forward-mode AD, we apply the chain rule to each function $ f_j $ and combine them in a recursion relationship. For the adjoint (reverse mode) AD, we construct the adjoint for each function $ f_j $ and combine them in reverse order.\n",
    "\n",
    "Let us describe the process using matrix notation. If we view the forward-mode AD as the result of the multiplication of a series of Jacobian matrices:\n",
    "\n",
    "$$ J = J_1 \\times J_2 \\times \\cdots \\times J_p $$\n",
    "\n",
    "where each Jacobian matrix $ J_j $ represents either a subroutine or a single statement, then the adjoint approach can be viewed as a product of adjoint subproblems:\n",
    "\n",
    "$$ J^T = J_p^T \\times J_{p-1}^T \\times \\cdots \\times J_1^T $$\n",
    "\n",
    "## Example\n",
    "Let us illustrate this with an example for $ p = 2, m=1 $. The computational flow is described by the following diagram:\n",
    "\n",
    "$$ x \\rightarrow f_1(x) \\rightarrow f_2(f_1(x)) \\rightarrow y $$\n",
    "\n",
    "For simplicity, let $ a $ denote the output of $ f_1(x) $ and $ b $ denote the output of $ f_2(f_1(x)) $. The scalar $ y $ is the final output.\n",
    "\n",
    "With these notations, we have:\n",
    "\n",
    "$$ x \\rightarrow a \\rightarrow b \\rightarrow y $$\n",
    "\n",
    "### Forward Mode AD\n",
    "In the AD literature we use dot to represent the partial wrt the input i.e. $\\dot{u}=\\frac{\\partial u}{\\partial x}$.\n",
    "Applying the tangent linear methodology (essentially differentiation line by line), we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dot{a} &= \\frac{\\partial a}{\\partial x} \\dot{x} \\\\\n",
    "    \\dot{b} &= \\frac{\\partial b}{\\partial a} \\dot{a} \\\\\n",
    "    \\dot{y} &= \\frac{\\partial y}{\\partial b} \\dot{b}\n",
    "\\end{align*}\n",
    "\n",
    "Putting everything together, we get:\n",
    "\n",
    "$$ \\dot{y} = \\frac{\\partial y}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x} \\dot{x} $$\n",
    "\n",
    "<!-- TODO make a point that you have to do this n times for R^n -->\n",
    "\n",
    "### Reverse Mode AD\n",
    "Using notation from AD literature, the adjoint quantities $ \\bar{x}, \\bar{a}, \\bar{b}, \\bar{y} $ denote the derivatives of $ y $ with respect to $ x, a, b $ and, respectively, to $ y $. We note that this implies $ \\bar{y} = 1 $.\n",
    "\n",
    "Differentiating again, and with a superscript $ T $ denoting a matrix or vector transpose, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{x} &= \\left( \\frac{\\partial y}{\\partial x} \\right)^T = \\left( \\frac{\\partial y}{\\partial a} \\frac{\\partial a}{\\partial x} \\right)^T = \\left( \\frac{\\partial a}{\\partial x} \\right)^T \\bar{a} \\\\\n",
    "    \\bar{a} &= \\left( \\frac{\\partial y}{\\partial a} \\right)^T = \\left( \\frac{\\partial y}{\\partial b} \\frac{\\partial b}{\\partial a} \\right)^T = \\left( \\frac{\\partial b}{\\partial a} \\right)^T \\bar{b} \\\\\n",
    "    \\bar{b} &= \\left( \\frac{\\partial y}{\\partial b} \\right)^T = \\left( \\frac{\\partial y}{\\partial b} \\right)^T \\cdot 1 = \\left( \\frac{\\partial y}{\\partial b} \\right)^T \\bar{y}\n",
    "\\end{align*}\n",
    "\n",
    "Putting everything together, we get:\n",
    "\n",
    "$$ \\bar{x} = \\left( \\frac{\\partial a}{\\partial x} \\right)^T \\left( \\frac{\\partial b}{\\partial a} \\right)^T \\left( \\frac{\\partial y}{\\partial b} \\right)^T \\bar{y} $$\n",
    "\n",
    "We notice that the forward mode AD proceeds forward through the process:\n",
    "\n",
    "$$ \\dot{x} \\rightarrow \\dot{a} \\rightarrow \\dot{b} \\rightarrow \\dot{y} $$\n",
    "\n",
    "while the adjoint (reverse mode) AD proceeds backward through the process:\n",
    "\n",
    "$$ \\bar{x} \\leftarrow \\bar{a} \\leftarrow \\bar{b} \\leftarrow \\bar{xy} $$\n",
    "\n",
    "In summary, reverse-mode AD involves calculating the adjoints of the output with respect to each intermediate variable by propagating the derivatives backward through each function in the composite function chain. This is efficient when the number of input variables $ n $ is large compared to the number of output variables $ m $.\n",
    "\n",
    "From now on we will only focus on reverse mode AD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dff17c9",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "## Small example\n",
    "We begin with an example where we want to compute the gradient of the function $d: \\mathbb{R}^2 \\to \\mathbb{R}$, $c: \\mathbb{R}^2 \\to \\mathbb{R}$ with respect to $a\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$. The point of this exercise is to show that you can use a computational graph directly to state the gradient. Given the function\n",
    "\n",
    "\\begin{align*}\n",
    "    a &= 4\n",
    "    \\\\\n",
    "    b &= 3\n",
    "    \\\\\n",
    "    c(a,b) &= a + b\n",
    "    \\\\\n",
    "    d(a,b) &= a c(a,b)\n",
    "\\end{align*}\n",
    "\n",
    "### Adjoint Calculations (Backward pass for a)\n",
    "\n",
    "1. Initialize the adjoint for the output $ d $:\n",
    "   $ \\bar{d} = 1 $\n",
    "\n",
    "2. Compute the adjoint for $ c $:\n",
    "\n",
    "   $ \\bar{c} = \\bar{d} \\cdot \\frac{\\partial d}{\\partial c} $,\n",
    "   $ \\frac{\\partial d}{\\partial c} = a $,\n",
    "   $ \\bar{c} = 1 \\cdot 4 = 4 $\n",
    "\n",
    "3. Compute the adjoint for $ a $:\n",
    "\n",
    "   There are two contributions to $\\bar{a}$:\n",
    "\n",
    "   $ \\bar{a} = \\bar{d} \\cdot \\frac{\\partial d}{\\partial a} + \\bar{c} \\cdot \\frac{\\partial c}{\\partial a} $\n",
    "\n",
    "   - From $ d $:\n",
    "     $ \\frac{\\partial d}{\\partial a} = c $\n",
    "     $ \\bar{a}_{(d)} = 1 \\cdot 7 = 7 $\n",
    "\n",
    "   - From $ c $:\n",
    "     $ \\frac{\\partial c}{\\partial a} = 1 $\n",
    "     $ \\bar{a}_{(c)} = 4 \\cdot 1 = 4 $\n",
    "\n",
    "   Combining both contributions:\n",
    "   $ \\bar{a} = 7 + 4 = 11 $\n",
    "\n",
    "## Generalized AD\n",
    "The specific example above shows how to apply backward AD for this particular example. However, we can generalize this process by building a computational graph of the function $d$ shown in the figure below:\n",
    "\n",
    "<div id=\"fig:auto-diff-simple-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-simple-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 1:  The function $d$ is composed of basic operations (addition, multiplication). By representing the function as a graph, we can visualize the flow of information through the function. When computing gradients, we start at the end of the graph and work our way backwards, multiplying the gradients of the children and adding the branches.</i></p>\n",
    "\n",
    "The function $d$ is composed of basic operations (addition $u_1$, multiplication $u_2$). By representing the function as a graph, we can visualize the flow of information through the function. When computing the gradient, we start at the end of the graph and work our way backwards, multiplying the adjoints of the children and adding the branches. During the forward pass the nodes store information about the actual values and the adjoints.\n",
    "\n",
    "First we let $x\\in\\mathbb{R}^2$ represent the input $x=(a,b)$ such that\n",
    "\n",
    "$$ \\bar{x} \\leftarrow \\bar{u_1} \\leftarrow \\bar{u_2} \\leftarrow \\bar{d} $$\n",
    "where $\\bar{d}=1$. Work through this example and find the gradient $\\bar{x} = \\nabla_x d$ using the graph. On the forward pass $u_1=c=a+b=7$ and $u_2=ac=7\\times4=28$ are stored alongside the adjoints\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{u_1}\n",
    "    &= (\\frac{\\partial u_1}{\\partial a}, \\frac{\\partial u_1}{\\partial b})\\\\\n",
    "    &= (1, 1)\\\\\n",
    "\\bar{u_2}\n",
    "    &= (\\frac{\\partial u_2}{\\partial a}, \\frac{\\partial u_2}{\\partial c})\\\\\n",
    "    &= (c, a)\\\\\n",
    "    &= (7, 4)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore $\\frac{\\partial d}{\\partial a} = \\frac{\\partial u_2}{\\partial c}\\frac{\\partial u_1}{\\partial a} + \\frac{\\partial u_2}{\\partial a} = 4\\times1+7=11, \\frac{\\partial d}{\\partial b}= \\frac{\\partial u_2}{\\partial c}\\frac{\\partial u_1}{\\partial b} = 4\\times1$ which means $\\nabla d = (11, 4)$\n",
    "\n",
    "Computing it directly using $d = a^2 + ab$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial d}{\\partial a} &= 2a + b = 11\n",
    "    \\\\\n",
    "    \\frac{\\partial d}{\\partial b} &= a = 4\n",
    "\\end{align*}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c8e43e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Compute the partial derivatives automatically\n",
    "\n",
    "Work through this simple example and implement a minimal version of a variable class and a method to compute gradients.\n",
    "\n",
    "To do so we have created a `Variable` class that stores the value of the variable and the gradients with respect to its children.\n",
    "The gradients are stored as a tuple of tuples, where each tuple contains a reference to the child variable and it correspoding partial\n",
    "derivative.\n",
    "\n",
    "Fill in the missing code in the `mul` function to compute the gradients of the variables with respect to their children.\n",
    "Then fill in the missing code in the `compute_gradients` function to compute the gradients of the variables with respect to their children."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e11d1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fb3f6e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        return compute_gradients(self)\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # d/da(a+b) is 1\n",
    "        (b, 1)   # d/db(a+b) is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    # ---> TODO: fill in the missing code <---\n",
    "    value = ___\n",
    "    gradients = (\n",
    "        (a, ___), # d/da(ab) is ____\n",
    "        (b, ___)  # d/db(ab) is ____\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # ---> TODO: fill in the missing code <---\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = ___\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] = ___\n",
    "\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd6bb97",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        if self._stored_gradients is None:\n",
    "            self._stored_gradients = dict(compute_gradients(self))\n",
    "        return self._stored_gradients\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # d/da(a+b) is 1\n",
    "        (b, 1)   # d/db(a+b) is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    value = a.value * b.value\n",
    "    gradients = (\n",
    "        (a, b.value), # d/da(ab) b.value\n",
    "        (b, a.value)  # d/db(ab) is a.value\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = total_gradient * child_gradient\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] += gradient\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f68c5695",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Test function against true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a74e83f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "# TODO: fill in the missing code\n",
    "b = ___\n",
    "c = ___\n",
    "d = ___\n",
    "\n",
    "assert d.gradients[a] == ___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac99058",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "b = Variable(3)\n",
    "c = add(a, b)\n",
    "d = mul(a, c)\n",
    "\n",
    "assert d.gradients[a] == 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5631591d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear rate neurons\n",
    "\n",
    "Starting with the loss $\\mathcal{L}=\\frac{1}{2}(y - \\hat{y})^2$ we have the computation graph in the figure below.\n",
    "\n",
    "<!-- dom:FIGURE: [figures/auto-diff-graph.png] <div id=\"fig:auto-diff-graph\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:auto-diff-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 2</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de4416f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Validate the gradients\n",
    "\n",
    "In this exercise we want to show the benefit of autodiff. \n",
    "In the previous module: LinearRegression, we saw that we had to compute gradients of the function manually - this is what autodiff is gonna automate for us. \n",
    "We will simply define the forward pass of our function (**fill in the blanks `___` in the functions `loss`, `sigma` and `y_hat` below**).\n",
    "In this exercise we will use a predefined Variable class from `variable.py` to automatically compute its gradient. \n",
    "We will compare with the ground truth analytical gradient (we restrict ourselves to compare the partial derivative of one variable $w_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449fa300",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import a \"complete\" version of the above Variable class.\n",
    "from variable import *\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def sigma(x):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def y_hat(w_0, w_1, x_1):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def dy_hat_dw_1(w_0, w_1, x_1):\n",
    "    return x_1*y_hat(w_0, w_1, x_1) * (1 - y_hat(w_0, w_1, x_1))\n",
    "\n",
    "def dloss_dw_1(y, w_0, w_1, x_1):\n",
    "    return (y_hat(w_0, w_1, x_1) - y) * dy_hat_dw_1(w_0, w_1, x_1)\n",
    "\n",
    "x_1 = Variable(0.1, name='x_1')\n",
    "w_0 = Variable(4, name='w_0')\n",
    "w_1 = Variable(3, name='w_1')\n",
    "y = Variable(10, name='y')\n",
    "\n",
    "def isclose(a, b):\n",
    "    \"\"\"\n",
    "    Checks if two variables are similiar\n",
    "    \"\"\"\n",
    "    a = a if not isinstance(a, Variable) else a.value\n",
    "    b = b if not isinstance(b, Variable) else b.value\n",
    "    return np.isclose(a, b)\n",
    "\n",
    "# Test if analytic and autodiff for computing d_loss/d_w_1 is similar\n",
    "assert isclose(loss(y, y_hat(w_0, w_1, x_1)).gradients[w_1], dloss_dw_1(y, w_0, w_1, x_1))\n",
    "print('success!!!!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989d4cd2",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# import a \"complete\" version of the above Variable class.\n",
    "from variable import *\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return 0.5 * (y - y_hat)**2\n",
    "\n",
    "def sigma(x):\n",
    "    return 1. / (1. + exp(-x))\n",
    "\n",
    "def y_hat(w_0, w_1, x_1):\n",
    "    return sigma(w_0 + w_1*x_1)\n",
    "\n",
    "def dy_hat_dw_1(w_0, w_1, x_1):\n",
    "    return x_1*y_hat(w_0, w_1, x_1) * (1 - y_hat(w_0, w_1, x_1))\n",
    "\n",
    "def dloss_dw_1(y, w_0, w_1, x_1):\n",
    "    return (y_hat(w_0, w_1, x_1) - y) * dy_hat_dw_1(w_0, w_1, x_1)\n",
    "\n",
    "x_1 = Variable(0.1, name='x_1')\n",
    "w_0 = Variable(4, name='w_0')\n",
    "w_1 = Variable(3, name='w_1')\n",
    "y = Variable(10, name='y')\n",
    "\n",
    "def isclose(a, b):\n",
    "    \"\"\"\n",
    "    Checks if two variables are similiar\n",
    "    \"\"\"\n",
    "    a = a if not isinstance(a, Variable) else a.value\n",
    "    b = b if not isinstance(b, Variable) else b.value\n",
    "    return np.isclose(a, b)\n",
    "\n",
    "# Test if analytic and autodiff for computing d_loss/d_w_1 is similar\n",
    "assert isclose(loss(y, y_hat(w_0, w_1, x_1)).gradients[w_1], dloss_dw_1(y, w_0, w_1, x_1))\n",
    "print('success!!!!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doconce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
