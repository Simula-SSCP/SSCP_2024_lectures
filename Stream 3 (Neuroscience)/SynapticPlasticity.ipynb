{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81c271f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb SynapticPlasticity.do.txt  -->\n",
    "\n",
    "## Short term plasticity\n",
    "Facilitation and depression can both be modeled as \n",
    "presynaptic processes that modify the probability of transmitter release. For both facilitation and depression, \n",
    "the release probability after a long period of presynaptic silence is $P_{rel} = P_0$. \n",
    "Activity at the synapse causes $P_{rel}$ to increase in the case of facilitation and to decrease for depression. \n",
    "Between presynaptic action potentials, the release probability decays exponentially back to its 'resting' value $P_0$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaccef",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_P \\frac{dP_{rel}}{dt} = P_0 - P_{rel}\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e90945",
   "metadata": {
    "editable": true
   },
   "source": [
    "The parameter $\\tau_P$ controls the rate at which the release probability decays to $P_0$. \n",
    "The models of facilitation and depression differ in how the release probability is changed by presynaptic activity. \n",
    "In the case of facilitation, $P_{rel}$ is augmented by making the replacement \n",
    "$P_{rel} \\rightarrow P_{rel} + f_F (1 - P_{rel})$ immediately after a presynaptic action potential. \n",
    "The parameter $f_F$ (with $0 \\leq f_F \\leq 1$) controls the degree of facilitation, and the factor $(1 - P_{rel})$ \n",
    "prevents the release probability from growing larger than one. To model depression, the release probability is reduced \n",
    "after a presynaptic action potential by making the replacement $P_{rel} \\rightarrow f_D P_{rel}$. \n",
    "In this case, the parameter $f_D$ (with $0 \\leq f_D \\leq 1$) controls the amount of depression, and the \n",
    "factor $P_{rel}$ prevents the release probability from becoming negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32149b69",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Facilitation and depression in Brian2\n",
    "\n",
    "In this exercise, you will implement a model of facilitation and depression in Brian2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f432560",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**\n",
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c52d612",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "start_scope()\n",
    "\n",
    "# Simulation parameters\n",
    "duration = 0.5*second\n",
    "dt = 0.001*ms\n",
    "defaultclock.dt = dt\n",
    "\n",
    "# Neuron parameters\n",
    "u_rest = -65*mV  # mV\n",
    "g_l = 10*uS\n",
    "C_m = 200*pF\n",
    "\n",
    "# Synapse parameters\n",
    "E_s = 0*mV\n",
    "tau_s = 1*ms\n",
    "tau_p = 100*ms\n",
    "g_s_bar = 10*nS\n",
    "ff = 0.5  # factor for synaptic facilitation\n",
    "# fd = 0.5  # factor for synaptic depression\n",
    "P0 = 0.1\n",
    "Ps_max = 1\n",
    "\n",
    "eqs = '''\n",
    "du/dt = code here / C_m : volt\n",
    "g : siemens\n",
    "'''\n",
    "model = '''\n",
    "dPrel/dt = code here: 1 (clock-driven)\n",
    "dPs/dt = code here : 1 (clock-driven)\n",
    "g_post = code here : siemens (summed)\n",
    "'''\n",
    "neurons = NeuronGroup(1, eqs, method='euler')\n",
    "neurons.u = u_rest\n",
    "times = arange(10,200,20)*ms  # Set up your spike times here\n",
    "indices = array([0]*len(times))\n",
    "spikes = SpikeGeneratorGroup(1, indices, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089de7cc",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eqs = '''\n",
    "du/dt = (g_l*(u_rest - u) - g*u_rest) / C_m : volt\n",
    "g : siemens\n",
    "'''\n",
    "model = '''\n",
    "dPrel/dt = (P0 - Prel)/tau_p : 1 (clock-driven)\n",
    "dPs/dt = - Ps / tau_s : 1 (clock-driven)\n",
    "g_post = g_s_bar*Prel*Ps : siemens (summed)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331d8bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Define the synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144db7a5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synapse = Synapses(spikes, neurons, model, on_pre='code here')\n",
    "# synapse = Synapses(spikes, neurons, model, on_pre='code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9596e943",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synapse = Synapses(spikes, neurons, model, on_pre='Prel += ff*(1-Prel); Ps += Ps_max*(1-Ps)')\n",
    "# synapse = Synapses(spikes, neurons, model, on_pre='Prel = fd*Prel; Ps += Ps_max*(1-Ps)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbe22c0",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "synapse.connect()\n",
    "synapse.Prel = 0.1\n",
    "\n",
    "# Monitor membrane potential\n",
    "monitor = StateMonitor(neurons, ['u', 'g'], record=True)\n",
    "monitor_s = StateMonitor(synapse, ['Ps', 'Prel'], record=True)\n",
    "\n",
    "# Run simulation\n",
    "run(duration)\n",
    "\n",
    "figure(figsize=(9, 4))\n",
    "plot(monitor.t/ms, monitor.g[0]/nS)\n",
    "xlabel('Time (ms)')\n",
    "ylabel('Total conductance (nS)')\n",
    "figure(figsize=(9, 4))\n",
    "plot(monitor.t/ms, monitor.u[0]/mV)\n",
    "xlabel('Time (ms)')\n",
    "ylabel('Membrane potential (mV)')\n",
    "\n",
    "figure(figsize=(9, 4))\n",
    "plot(monitor.t/ms, monitor_s.Prel[0])\n",
    "xlabel('Time (ms)')\n",
    "ylabel('Release probability')\n",
    "figure(figsize=(9, 4))\n",
    "plot(monitor.t/ms, monitor_s.Ps[0])\n",
    "xlabel('Time (ms)')\n",
    "ylabel('Post-synaptic Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee66180",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plasticity rules\n",
    "\n",
    "Here we explore activity-dependent synaptic plasticity, focusing on Hebbian type and its augmentation with more \n",
    "global synaptic modifications. Non-Hebbian synaptic plasticity, which modifies synaptic strengths based solely on \n",
    "pre- or postsynaptic firing, is emphasized as an important factor in homeostatic, developmental, and learning processes. \n",
    "Furthermore, we also explore the influence of activity on intrinsic excitability and response properties of neurons, and \n",
    "the interplay of intrinsic and synaptic plasticity.\n",
    "\n",
    "Hebbian plasticity leads to an increasing synaptic strength, but without constraints it can result in uncontrolled \n",
    "growth of synaptic strengths. An upper limit on synaptic weight can serve as a control measure, supported by LTP experiments. \n",
    "To this end one can impose a saturation constraint ensuring that all excitatory synaptic weights lie between zero \n",
    "and a maximum constant value, $w_{max}$. \n",
    "\n",
    "Adequate synaptic development typically requires competition between different synapses, prompting some to weaken \n",
    "when others strengthen. We discuss several synaptic plasticity rules introducing such competition.\n",
    "\n",
    "Synaptic plasticity rules take the form of differential equations, where the rate of change of synaptic weights \n",
    "depends on the pre- and postsynaptic activity. In the models studied, neuronal activity is represented by a continuous \n",
    "variable, $u$ and $v$ for presynaptic and postsynaptic activity respectively. \n",
    "\n",
    "In the initial segment, we explore the application of unsupervised learning to a single postsynaptic neuron that is \n",
    "influenced by $\\text{Nu}$ presynaptic inputs. \n",
    "The activities of these inputs are denoted by $u_b$ for $b = 1, 2, \\ldots, \\text{Nu}$ or, collectively, \n",
    "by the vector $\\mathbf{u}$. As the learning is unsupervised, the postsynaptic activity $\\mathbf{v}$ is \n",
    "directly prompted by the presynaptic activity $\\mathbf{u}$, not by an external entity.\n",
    "\n",
    "We utilize a linear variant of the firing-rate model, which can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723794a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rate_model\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_r \\frac{dv}{dt} = -v + \\mathbf{w} \\cdot \\mathbf{u} = -v + \\sum_{b=1}^{\\text{Nu}} w_b u_b\n",
    "\\label{eq:rate_model} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46f231",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\tau_r$ is a time constant which manages the dynamics of the firing rate response. \n",
    "Moreover, $w_b$ denotes the synaptic weight, describing the strength of the synapse from the \n",
    "presynaptic neuron $b$ to the postsynaptic neuron, and $\\mathbf{w}$ is the vector comprising all $\\text{Nu}$ synaptic weights.\n",
    "These synaptic weights can be positive, indicating excitation, or negative, indicating inhibition. \n",
    "Equation ([2](#eq:rate_model)) does not encompass any non-linear dependence of the firing rate on the total synaptic input, \n",
    "including rectification.\n",
    "\n",
    "Adopting such a linear firing-rate model significantly streamlines the analysis of synaptic plasticity. \n",
    "The limitation to non-negative $\\mathbf{v}$ will be either imposed manually, or sometimes overlooked to simplify \n",
    "the analysis.\n",
    "\n",
    "The mechanisms of synaptic plasticity are usually much slower than the dynamics outlined by equation ([2](#eq:rate_model)). \n",
    "Furthermore, if the stimuli are introduced slowly enough to enable the network to reach its steady-state activity \n",
    "during training, we can substitute the dynamic equation ([2](#eq:rate_model)) by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0924e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:rate_model_ss\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "v = \\mathbf{w} \\cdot \\mathbf{u}\n",
    "\\label{eq:rate_model_ss} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b494017",
   "metadata": {
    "editable": true
   },
   "source": [
    "This equation instantaneously sets $v$ to the asymptotic, steady-state value determined by equation ([2](#eq:rate_model)). \n",
    "This is the primary equation we utilize in our analysis of synaptic plasticity in unsupervised learning.\n",
    "\n",
    "Synaptic modifications are included in the model by designating how the vector $\\mathbf{w}$ alters as a function of \n",
    "the pre- and postsynaptic levels of activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51da46e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Simplest Plasticity Rule: Basic Hebb Rule\n",
    "\n",
    "The simplest form of plasticity rule that is consistent with Hebb's conjecture is given by the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0db488",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:basic_hebb\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = v\\mathbf{u}\n",
    "\\label{eq:basic_hebb} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90c5bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "This implies that the simultaneous firing of pre- and postsynaptic neurons enhances synaptic strength, \n",
    "which we refer to as the basic Hebb rule. \n",
    "If the activity variables symbolize firing rates, the right-hand side of this equation could be viewed as a \n",
    "measure of the likelihood of the pre- and postsynaptic neurons both firing spikes during a small time interval. \n",
    "Here, $\\tau_w$ is a time constant that regulates the rate of weight changes.\n",
    "\n",
    "Synaptic plasticity is generally modeled as a slow process, where the input pattern $u$ take on a variety of values. \n",
    "To calculate the weight changes induced by a series of input patterns we average over all \n",
    "different input patterns and calculate the weight changes induced by this average. \n",
    "As long as the synaptic weights change slowly enough, this averaging method provides a \n",
    "good approximation of the weight changes produced by the set of input patterns.\n",
    "\n",
    "We use angle brackets $\\langle \\rangle$ to denote averages over the ensemble of input patterns presented during \n",
    "training. \n",
    "The Hebb rule of Eq. ([4](#eq:basic_hebb)), when averaged over the inputs used during training, becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60aae1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:basic_hebb_averaged\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = \\langle v\\mathbf{u} \\rangle\n",
    "\\label{eq:basic_hebb_averaged} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33609a",
   "metadata": {
    "editable": true
   },
   "source": [
    "We replace $v$ by $\\mathbf{w} \\cdot \\mathbf{u}$, and introduce $\\mathbf{Q}$ as the input correlation \n",
    "matrix defined by $\\mathbf{Q} = \\langle \\mathbf{u}\\mathbf{u} \\rangle$. \n",
    "With this we can rewrite the averaged plasticity rule Eq. ([5](#eq:basic_hebb_averaged)) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97af7fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:basic_hebb_averaged_2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} &= \\mathbf{Q} \\cdot \\mathbf{w}\n",
    "\\label{eq:basic_hebb_averaged_2} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb49b55",
   "metadata": {
    "editable": true
   },
   "source": [
    "Equation ([6](#eq:basic_hebb_averaged_2)) is referred to as a correlation-based plasticity \n",
    "rule due to the presence of the input correlation matrix.\n",
    "\n",
    "The Hebb rule, tends to cause unbounded weight growth because it lacks an upper limit and \n",
    "fails to generate competition among different synapses. \n",
    "This can be demonstrated by examining the square of the weight vector length and its \n",
    "change over time yielding $\\tau_w \\frac{d|\\mathbf{w}|^2}{dt} = 2v^2$, which is always positive \n",
    "and the length of the weight vector continuously grows. A method to control this growth is by \n",
    "introducing an upper saturation constraint and a lower limit if the activity variables can \n",
    "be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbf873",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Covariance Rule\n",
    "\n",
    "The Covariance Rule is another synaptic plasticity rule which models the way synaptic \n",
    "strength can increase or decrease depending on the level of postsynaptic activity. \n",
    "It is represented by the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dfdf05",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = (v - \\theta_v)\\mathbf{u}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd01fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\theta_v$ is a threshold that determines the level of postsynaptic activity \n",
    "above which long-term depression switches to long-term potentiation. \n",
    "Alternatively, we can impose the threshold on the input activity instead of the \n",
    "output activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8a7d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = v(\\mathbf{u} - \\mathbf{\\theta_u})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9a152",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case, $\\mathbf{\\theta_u}$ is a vector of thresholds that determines the levels of \n",
    "presynaptic activities above which LTD switches to LTP. \n",
    "The thresholds are usually set to the average value of the corresponding variable \n",
    "over the training period. These equations are known as covariance rules because of the \n",
    "presence of the covariance matrix in the averaged form of the plasticity rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888121f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = \\mathbf{C} \\cdot \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f2b5c",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\mathbf{C}$ is the covariance matrix defined as \n",
    "$\\mathbf{C} = \\langle \\mathbf{u} \\mathbf{u} \\rangle - \\langle \\mathbf{u} \\rangle \\langle \\mathbf{u} \\rangle$.\n",
    "\n",
    "Although the covariance rules include long-term depression, they are unstable due to \n",
    "the same positive feedback that makes the basic Hebb rule unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810e8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The BCM Rule\n",
    "Finally, there's the BCM rule which requires both pre- and postsynaptic activity to \n",
    "change a synaptic weight. It's formulated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51fd2d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_w \\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = v\\mathbf{u}(v - \\theta_v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa37e72",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case, $\\theta_v$ is a variable threshold on the postsynaptic activity that determines \n",
    "whether synapses are strengthened or weakened. \n",
    "The critical condition for stability is that $\\theta_v$ must grow faster than $v$ \n",
    "if the output activity grows large.\n",
    "The stability of the BCM rule can be achieved, in one implementation, \n",
    "where the threshold follows the square of the postsynaptic activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38d70c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_\\theta \\frac{d\\theta_v}{dt} = v^2 - \\theta_v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da181cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\tau_\\theta$ sets the time scale for the modification of the threshold. \n",
    "The BCM rule implements competition between synapses because strengthening some \n",
    "synapses increases the postsynaptic firing rate, which raises the threshold and \n",
    "makes it more difficult for other synapses to be strengthened or remain at their \n",
    "current strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84408134",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Implement the BCM rule\n",
    "\n",
    "Plot the evolution of weights and theta over time\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12bf337",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def BCM(pre_syn_activity, post_syn_activity, weights, tau_w=0.1, tau_theta=0.1, learning_rate=0.01):\n",
    "    # Initialize threshold\n",
    "    theta_v = 0.5\n",
    "    \n",
    "    num_iterations = len(pre_syn_activity)\n",
    "    \n",
    "    # We keep track of the evolution of weights and theta\n",
    "    weight_evolution = np.zeros(num_iterations)\n",
    "    theta_evolution = np.zeros(num_iterations)\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Calculate the weight update according to the BCM rule\n",
    "        dw = post_syn_activity[t] * pre_syn_activity[t] * (post_syn_activity[t] - theta_v)\n",
    "        \n",
    "        # Update the weight\n",
    "        weights += learning_rate * dw / tau_w\n",
    "        \n",
    "        # Update the threshold\n",
    "        theta_v += learning_rate * (post_syn_activity[t]**2 - theta_v) / tau_theta\n",
    "        \n",
    "        # Store the weights and theta values\n",
    "        weight_evolution[t] = weights\n",
    "        theta_evolution[t] = theta_v\n",
    "    \n",
    "    return weights, weight_evolution, theta_evolution\n",
    "\n",
    "# Let's test the BCM rule with some data\n",
    "pre_syn_activity = np.random.rand(1000)\n",
    "post_syn_activity = np.random.rand(1000)\n",
    "initial_weights = 0.5\n",
    "\n",
    "final_weights, weight_evolution, theta_evolution = BCM(pre_syn_activity, post_syn_activity, initial_weights)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(weight_evolution)\n",
    "plt.title('Evolution of weights')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta_evolution)\n",
    "plt.title('Evolution of theta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f924609",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, pre_syn_activity and post_syn_activity represent the pre- and post-synaptic activities, respectively, and \n",
    "weights represent the synaptic weights. The function BCM updates the weights and the threshold (theta_v) \n",
    "according to the BCM rule over the course of several iterations (set by the length of pre_syn_activity). \n",
    "The learning rate and the time constants for the weights and the threshold can be set by learning_rate, \n",
    "tau_w, and tau_theta, respectively. The function returns the final weights and the evolution of weights and \n",
    "threshold over time.\n",
    "\n",
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d238da",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Weight Normalization\n",
    "The BCM rule achieves stabilization of Hebbian plasticity by employing a dynamic \n",
    "threshold that reduces synaptic weights when the postsynaptic neuron activity is \n",
    "excessively high, essentially using postsynaptic activity as an index of synaptic weight \n",
    "strengths. An alternative stabilization method introduces weight-dependent terms, leading \n",
    "to weight normalization, built on the assumption that postsynaptic neurons can only \n",
    "sustain a constant total synaptic weight, indicating that weight increases must be \n",
    "compensated by decreases elsewhere.\n",
    "\n",
    "Synaptic weight normalization involves applying a global constraint. Two types of \n",
    "constraints are typically employed. For non-negative synaptic weights, their growth can \n",
    "be bounded by maintaining a constant sum of all the weights of the synapses onto a \n",
    "given postsynaptic neuron. Alternatively, for weights that can be both positive or \n",
    "negative, one can constrain the sum of the squares of the weights rather than their \n",
    "linear sum. In both cases, the constraint can be enforced either strictly, requiring \n",
    "it to be met always during training, or dynamically, needing it to be met asymptotically \n",
    "at training's end.\n",
    "\n",
    "A constraint on the sum of the squares of the synaptic weights can be dynamically enforced \n",
    "using a modification of the basic Hebb rule, known as Oja's rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acb632",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ojas_rule\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau_w \\frac{d\\mathbf{w}}{dt} = vu - \\alpha v^2 w\n",
    "\\label{eq:ojas_rule} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071840b9",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, $\\alpha$ is a positive constant. This rule only involves local synapse information, \n",
    "i.e., the pre- and postsynaptic activities and the local synaptic weight. \n",
    "However, its derivation is more theory-based than empirical-data-based. \n",
    "The enforced normalization is termed multiplicative because the second term's \n",
    "modification in Eq. ([7](#eq:ojas_rule)) is proportional to $w$.\n",
    "\n",
    "The Oja rule's stability can be proven by computing the change of the weight vector length:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed9316",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tau_w \\frac{d|w|^2}{dt} = 2v^2 (1 - \\alpha|w|^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ff3cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "This shows that over time, $|w|^2$ converges to $\\frac{1}{\\alpha}$, \n",
    "effectively preventing the weights from growing indefinitely, hence ensuring stability. \n",
    "It also promotes weight competition because when one weight increases, the need to keep a \n",
    "constant weight vector length compels other weights to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac7eb6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Implement the Oja rule\n",
    "\n",
    "**a)**\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22cdcf9",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ojas_rule(x, w, alpha):\n",
    "    \"\"\"\n",
    "    x: input vector\n",
    "    w: weights vector\n",
    "    alpha: learning rate\n",
    "    \"\"\"\n",
    "    # calculate the dot product of x and w\n",
    "    v = np.dot(w, x)\n",
    "    \n",
    "    # update the weights\n",
    "    delta_w = alpha * v * (x - v * w)\n",
    "    \n",
    "    # return the updated weights\n",
    "    return w + delta_w\n",
    "\n",
    "# initialize weights\n",
    "w = np.random.rand(10)\n",
    "\n",
    "# define alpha\n",
    "alpha = 0.01\n",
    "\n",
    "# loop over some data (replace this with your actual data)\n",
    "for _ in range(1000):\n",
    "    # generate a random input\n",
    "    x = np.random.rand(10)\n",
    "    \n",
    "    # update weights\n",
    "    w = ojas_rule(x, w, alpha)\n",
    "\n",
    "# print final weights\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334a4e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function takes in an input vector x and the current weights vector w, and it returns the \n",
    "updated weights vector after applying the Oja rule.\n",
    "\n",
    "Note that the vector x and w should have the same dimensions, and alpha should be a small \n",
    "positive number (like 0.01). You might need to tune alpha based on your specific problem.\n",
    "\n",
    "You would typically call this function in a loop, passing in your inputs and current weights, \n",
    "and updating your weights with the returned value.\n",
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32450dcd",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Show that the Oja rule converges to a solution similar to the PCA solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821dc83",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Spike time dependent plasticity (STDP)\n",
    "Spike-Timing Dependent Plasticity (STDP) is a fundamental biological process responsible for \n",
    "synaptic modification, thus playing a critical role in the learning and memory functions of \n",
    "the brain. In neuroscience, plasticity refers to the brain's ability to change and adapt in \n",
    "response to experience. This is accomplished by adjusting the strength of connections between \n",
    "neurons, which is modulated by their synaptic weights. \n",
    "\n",
    "STDP is a form of Hebbian plasticity that further refines the \"fire together, wire together\" \n",
    "principle by introducing an element of causality based on the precise timing of spikes. \n",
    "In STDP, the change in synaptic strength depends not just on the simultaneous firing of \n",
    "the pre- and post-synaptic neurons, but also on the order and timing of these firing events.\n",
    "\n",
    "According to the rule of STDP, a synapse is strengthened if the presynaptic neuron fires \n",
    "just before the postsynaptic neuron (indicating that the presynaptic neuron might have \n",
    "contributed to the successful firing of the postsynaptic neuron). Conversely, if the \n",
    "presynaptic neuron fires just after the postsynaptic neuron, the synapse is weakened.\n",
    "\n",
    "This form of synaptic plasticity takes into account the temporal relationship between the \n",
    "firings of the pre- and post-synaptic neurons, making it a more dynamic and precise form of \n",
    "Hebbian learning. It provides a mechanism for temporal coding, a neural coding scheme where \n",
    "information is encoded in the precise timings of spikes.\n",
    "\n",
    "STDP has been observed in various types of neurons across many species, and has been shown to \n",
    "have significant effects on the learning and memory functions of neural circuits. \n",
    "Researchers continue to explore its implications for our understanding of the brain and \n",
    "for the development of neural network models in artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a228925",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 4: Implement STDP in brian2\n",
    "\n",
    "Exercise: Implementing Spike-Timing Dependent Plasticity (STDP) in Python\n",
    "\n",
    "Implement a simple model of a spiking neural network with Spike-Timing Dependent Plasticity \n",
    "(STDP) using the Brian2 library in Python. Consider a model with the following characteristics:\n",
    "\n",
    "1. There are 100 neurons in the network.\n",
    "2. Each neuron obeys the simple leaky integrate-and-fire model with an input current and a decay time constant of 10 ms.\n",
    "3. Neurons fire if their membrane potential exceeds 1 and reset to 0 after firing.\n",
    "4. Each neuron is connected to every other neuron in the network through synapses that \n",
    "exhibit STDP. The synaptic weights should change according to the timing difference between \n",
    "the spikes of the pre- and postsynaptic neurons.\n",
    "\n",
    "Implement the STDP model in Python using the Brian2 library and plot the synaptic weights, \n",
    "pre-synaptic activity, and post-synaptic activity as a function of time.\n",
    "\n",
    "The model of the neuron is defined by the following differential equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcca86c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{{dv}}{{dt}} = \\frac{{I-v}}{{10ms}}\n",
    "\\label{_auto2} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2f4a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $v$ is the membrane potential of the neuron and $I$ is the input current. \n",
    "Neurons fire if their membrane potential $v$ exceeds 1 and then $v$ is reset to 0.\n",
    "\n",
    "The model of STDP is defined by the following set of equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45809fb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "w : 1\n",
    "\n",
    "\n",
    "\\frac{{da_{pre}}}{{dt}} = -\\frac{{a_{pre}}}{{\\tau_{pre}}}\n",
    "\n",
    "\n",
    "\\frac{{da_{post}}}{{dt}} = -\\frac{{a_{post}}}{{\\tau_{post}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99be11",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $w$ is the synaptic weight, $a_{pre}$ is the pre-synaptic activity, $a_{post}$ is the \n",
    "post-synaptic activity, $\\tau_{pre}$ is the pre-synaptic time constant, and $\\tau_{post}$ \n",
    "is the post-synaptic time constant. \n",
    "\n",
    "The STDP rule is implemented in the on_pre and on_post sections, where $w$ is incremented \n",
    "by $a_{post}$ after a pre-synaptic spike and by $a_{pre}$ after a post-synaptic spike.\n",
    "\n",
    "Upon running the code, you should see the changes in synaptic weights, pre-synaptic \n",
    "activity, and post-synaptic activity over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18f862ae",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "\n",
    "# Parameters\n",
    "taupre = taupost = 20*ms\n",
    "Apre = 0.01\n",
    "Apost = -Apre*taupre/taupost*1.05\n",
    "tmax = 50*ms\n",
    "N = 100\n",
    "\n",
    "# Equations\n",
    "eqs_neurons = '''\n",
    "dv/dt = (I-v) / (10*ms) : 1 (unless refractory)\n",
    "I : 1\n",
    "'''\n",
    "###### STDP rule code here #######\n",
    "\n",
    "# Set up the monitors\n",
    "mon = StateMonitor(S, ['w', 'apre', 'apost'], record=[0, 1])\n",
    "\n",
    "# Run the simulation\n",
    "run(tmax)\n",
    "\n",
    "# Plot the results\n",
    "subplot(311)\n",
    "plot(mon.t/second, mon.w.T)\n",
    "ylabel('w')\n",
    "subplot(312)\n",
    "plot(mon.t/second, mon.apre.T)\n",
    "ylabel('apre')\n",
    "subplot(313)\n",
    "plot(mon.t/second, mon.apost.T)\n",
    "xlabel('Time (s)')\n",
    "ylabel('apost')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fbd770",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "\n",
    "# Parameters\n",
    "taupre = taupost = 20*ms\n",
    "Apre = 0.01\n",
    "Apost = -Apre*taupre/taupost*1.05\n",
    "tmax = 50*ms\n",
    "N = 100\n",
    "tau = 10*ms\n",
    "\n",
    "# Equations\n",
    "eqs_neurons = '''\n",
    "dv/dt = (I-v) / tau : 1 (unless refractory)\n",
    "I : 1\n",
    "'''\n",
    "eqs_synapses = '''\n",
    "w : 1\n",
    "dapre/dt = -apre/taupre : 1 (event-driven)\n",
    "dapost/dt = -apost/taupost : 1 (event-driven)\n",
    "'''\n",
    "on_pre = '''\n",
    "v_post += w\n",
    "apre += Apre\n",
    "w = clip(w+apost, 0, Inf)\n",
    "'''\n",
    "on_post = '''\n",
    "apost += Apost\n",
    "w = clip(w+apre, 0, Inf)\n",
    "'''\n",
    "\n",
    "# Set up the neurons\n",
    "G = NeuronGroup(N, eqs_neurons, threshold='v>1', reset='v=0', refractory=5*ms)\n",
    "G.I = linspace(1.51, 1.53, N)  # Different currents for different neurons\n",
    "\n",
    "H = NeuronGroup(N, eqs_neurons, threshold='v>1', reset='v=0', refractory=5*ms)\n",
    "H.I = linspace(1.51, 1.53, N)  # Different currents for different neurons\n",
    "# Set up the synapses\n",
    "S = Synapses(G, H, eqs_synapses,\n",
    "             on_pre=on_pre,\n",
    "             on_post=on_post)\n",
    "S.connect(j='i')\n",
    "\n",
    "# Set up the monitors\n",
    "mon = StateMonitor(S, ['w', 'apre', 'apost'], record=[0, 1])\n",
    "G_spikes = SpikeMonitor(G)\n",
    "H_spikes = SpikeMonitor(H)\n",
    "\n",
    "# Run the simulation\n",
    "run(tmax)\n",
    "\n",
    "# Plot the results\n",
    "subplot(311)\n",
    "plot(mon.t/second, mon.w.T)\n",
    "ylabel('w')\n",
    "subplot(312)\n",
    "plot(mon.t/second, mon.apre.T)\n",
    "ylabel('apre')\n",
    "subplot(313)\n",
    "plot(mon.t/second, mon.apost.T)\n",
    "xlabel('Time (s)')\n",
    "ylabel('apost')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
