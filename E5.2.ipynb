{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5.2 - Building ion current models from data \n",
    "\n",
    "In this exercise we will explore some of the challenges that come with optimizing a model to data. We'll use a moderately complex model for the ultra-rapidly activating delayed rectifier current ($I_{\\rm{Kur}}$). This model is simple in the sense that it has identical rate constants for each activating and deactivating transition, and it is linear. However, you'll see that there are still 12 parameters that can be varied during the fitting process, which can make for challenges in hand tuning the model to a simple activation curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a 5-state Markov model published by Zhou et al. 2012:\n",
    "\n",
    "<img src=\"fig/Model.svg\" width=400>\n",
    "\n",
    "Where: $$\\alpha{} = e^{\\frac{V-P1}{P2}}$$\n",
    "$$\\beta{} = \\frac{e^{\\frac{V-P3}{P4}}\\cdot{}e^{\\frac{-(V-P5)}{P6}}}{P7+P8\\cdot{}e^{\\frac{-(V-P9)}{P10}}}$$\n",
    "$$ K_1 = const_1 $$\n",
    "$$ K_2 = const_2 $$\n",
    "\n",
    "Below we will take some time to try to fit this model just to some activation data for this current. In this case we have also allowed the peak conductance ($g_K$) to be a free parameter for your tuning. See how close you can get to the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import scipy.optimize as opt\n",
    "import math\n",
    "from ObFunc import f, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "import K_widget as K\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(K)\n",
    "K.Markov_Widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see that this model is underdetermined and that there are far more parameters than necessary or helpful for fitting this activation curve. What do you think is the reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinatorial optimisation\n",
    "Next we will employ a commonly-used algorithm (the Nelder-Mead simplex, or AMOEBA) to optimise the parameter set, and see if it can beat you for accuracy. This algorithm is a form of derivative-free approach to optimise our voltage clamp activation function: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def Activation(P,V,duration):\n",
    "\n",
    "    'runs activation voltage protocol on the model'\n",
    "    'P is the vector of parameters [P1... P13]'\n",
    "    'V is the voltage at each step of the protocol [V1... V15]'\n",
    "    'duration is a variable specifying the length of each voltage clamp\n",
    "    step in time - 1000 ms is conventional'\n",
    "    \n",
    "    return out = {'t':t, 'Po':Po,'I_peak':model_peaks} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's specify the experimental data you will use for fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSA_data = np.loadtxt(\"SS.txt\",dtype='float')\n",
    "\n",
    "V = SSA_data[:,0] # voltage\n",
    "I = SSA_data[:,1] # current\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(V,I,'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to define a cost function for comparing the model generated data and the experimental data we have imported. See if you can create this function from the inputs and outputs prescribed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(P,V,I,duration):\n",
    "    \n",
    "    P = P.tolist() # Opt.fmin returns an array every iteration...\n",
    "                    # but we pass a list to Activation, so we need to typeset.\n",
    "    outs = Activation(P,V,duration)\n",
    "    model_peaks = outs['I_peak']\n",
    "       \n",
    "    dev_vector = model_peaks - I\n",
    "    error = np.linalg.norm(dev_vector,2) # calculating the error as the L2 norm\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimization and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_length = 1000\n",
    "init_params = [45,20,65,50,20,15,1,0.02,29,15,1e-5,1e-5,0.5] # a starting guess for the parameters\n",
    "\n",
    "[P_opt, f_opt, iters, funcalls, warnflag] = opt.fmin(cost, init_params, args=(V,I,step_length), maxiter = 300, maxfun = 300, full_output=True, disp=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally plot your optimized data compared to the experimental data below. Did the algorithm do better than your hand tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
